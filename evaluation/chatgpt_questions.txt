{
  "questions": [
  {
    "question": "What is the main idea behind Reinforcement Learning and how does it differ from supervised learning?",
    "blooms_taxonomy": "understand",
    "metadata": {
      "topic": "Introduction to RL",
      "subtopics": ["RL vs Supervised Learning", "Basic Concepts"]
    }
  },
  {
    "question": "Define the term 'Policy' in Reinforcement Learning. Why is it important?",
    "blooms_taxonomy": "remember",
    "metadata": {
      "topic": "Fundamentals",
      "subtopics": ["Policy", "Policy Importance"]
    }
  },
  {
    "question": "Describe the Bellman Equation for value functions.",
    "blooms_taxonomy": "understand",
    "metadata": {
      "topic": "Dynamic Programming",
      "subtopics": ["Bellman Equation", "Value Functions"]
    }
  },
  {
    "question": "What role does the discount factor play in RL algorithms?",
    "blooms_taxonomy": "understand",
    "metadata": {
      "topic": "Fundamentals",
      "subtopics": ["Discount Factor", "Future Rewards"]
    }
  },
  {
    "question": "Compare and contrast Value Iteration and Policy Iteration in Dynamic Programming.",
    "blooms_taxonomy": "analyze",
    "metadata": {
      "topic": "Dynamic Programming",
      "subtopics": ["Value Iteration", "Policy Iteration"]
    }
  },
  {
    "question": "Explain Temporal Difference (TD) Learning and how it combines Monte Carlo ideas and Dynamic Programming.",
    "blooms_taxonomy": "understand",
    "metadata": {
      "topic": "Temporal Difference Learning",
      "subtopics": ["TD Learning", "Monte Carlo", "DP"]
    }
  },
  {
    "question": "What are the advantages and limitations of Model-Free RL methods?",
    "blooms_taxonomy": "evaluate",
    "metadata": {
      "topic": "Model-Based vs Model-Free",
      "subtopics": ["Model-Free Advantages", "Limitations"]
    }
  },
  {
    "question": "Differentiate between Q-learning and SARSA algorithms.",
    "blooms_taxonomy": "analyze",
    "metadata": {
      "topic": "Temporal Difference Learning",
      "subtopics": ["Q-Learning", "SARSA"]
    }
  },
  {
    "question": "In what situations would you prefer an On-Policy method over an Off-Policy method?",
    "blooms_taxonomy": "apply",
    "metadata": {
      "topic": "RL Algorithms",
      "subtopics": ["On-Policy vs Off-Policy"]
    }
  },
  {
    "question": "Describe the exploration vs exploitation dilemma in Reinforcement Learning.",
    "blooms_taxonomy": "understand",
    "metadata": {
      "topic": "Exploration vs Exploitation",
      "subtopics": ["Balancing Strategies"]
    }
  },
  {
    "question": "What is the purpose of experience replay in Deep Reinforcement Learning?",
    "blooms_taxonomy": "understand",
    "metadata": {
      "topic": "Deep RL",
      "subtopics": ["Experience Replay"]
    }
  },
  {
    "question": "Explain how a Deep Q-Network (DQN) improves upon standard Q-learning.",
    "blooms_taxonomy": "understand",
    "metadata": {
      "topic": "Deep RL",
      "subtopics": ["DQN", "Q-Learning Improvement"]
    }
  },
  {
    "question": "What are eligibility traces and how do they accelerate learning?",
    "blooms_taxonomy": "understand",
    "metadata": {
      "topic": "Temporal Difference Learning",
      "subtopics": ["Eligibility Traces", "Speeding Up Learning"]
    }
  },
  {
    "question": "Summarize the concept of Policy Gradient methods in RL.",
    "blooms_taxonomy": "understand",
    "metadata": {
      "topic": "Policy Gradient Methods",
      "subtopics": ["Stochastic Policies", "Gradient Ascent"]
    }
  },
  {
    "question": "What problem does the Actor-Critic architecture aim to solve?",
    "blooms_taxonomy": "understand",
    "metadata": {
      "topic": "Policy Gradient Methods",
      "subtopics": ["Actor-Critic", "Variance Reduction"]
    }
  },
  {
    "question": "Explain the intuition behind Proximal Policy Optimization (PPO) algorithms.",
    "blooms_taxonomy": "understand",
    "metadata": {
      "topic": "Advanced RL Algorithms",
      "subtopics": ["PPO", "Policy Stability"]
    }
  },
  {
    "question": "How does Trust Region Policy Optimization (TRPO) differ from PPO?",
    "blooms_taxonomy": "analyze",
    "metadata": {
      "topic": "Advanced RL Algorithms",
      "subtopics": ["TRPO vs PPO"]
    }
  },
  {
    "question": "Discuss how Multi-Agent Reinforcement Learning differs from Single-Agent RL.",
    "blooms_taxonomy": "analyze",
    "metadata": {
      "topic": "Multi-Agent RL",
      "subtopics": ["Single vs Multi-Agent", "Coordination"]
    }
  },
  {
    "question": "Why is credit assignment a challenging problem in RL, and how is it handled?",
    "blooms_taxonomy": "understand",
    "metadata": {
      "topic": "Challenges in RL",
      "subtopics": ["Credit Assignment Problem", "Solutions"]
    }
  },
  {
    "question": "What are the main challenges when applying RL to real-world robotics?",
    "blooms_taxonomy": "evaluate",
    "metadata": {
      "topic": "Applications",
      "subtopics": ["Real-World Robotics", "Practical Challenges"]
    }
  }
]
}
