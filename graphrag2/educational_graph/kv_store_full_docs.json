{
  "doc-5e54a0db75e608a382c1ce071f140ac3": {
    "content": "SUBJECT: ADDO8013 Reinforcement Learning  \nDESCRIPTION: This course provides an in-depth exploration of reinforcement learning (RL). It covers the foundational concepts, algorithms, and diverse applications of RL. The primary emphasis is on understanding the role of rewards, decision-making processes through Markov decision processes, and the application of advanced RL algorithms to real-world problems.\n\nTOPICS:  \n0. Prerequisite  \n   Description: Basic knowledge of probability distributions, expected values, and fundamental linear algebra concepts such as inner products.\n\n1. Introduction to Reinforcement Learning  \n   Description: A foundational overview of Reinforcement Learning, its key features, and elements, including types of RL and the role of rewards.  \n   Learning Objectives: Understand the core concepts of Reinforcement Learning and differentiate between various RL algorithms.  \n   \n   Subtopics:  \n   1.1 Reinforcement Learning Algorithms  \n       Q-Learning, State Action Reward State action (SARSA).\n\n2. Bandit Problems and Online Learning  \n   Description: Introduction to bandit problems and methodologies used in online learning contexts.  \n   Learning Objectives: Develop the ability to solve n-Armed Bandit Problems and implement action-value methods.  \n\n   Subtopics:  \n   2.1 An n-Armed Bandit Problem  \n       Discuss approaches to solving this fundamental problem in decision making.  \n   2.2 Action-Value Methods  \n       Techniques for tracking nonstationary problems.  \n   2.3 Optimistic Initial Values  \n       Strategies for effective action selection using optimistic initial values.  \n   2.4 Upper-Confidence-Bound Action Selection  \n       Learn about using confidence bounds to inform decision making.  \n   2.5 Gradient Bandits  \n       Implementing gradient-based approaches for bandit problems.\n\n3. Markov Decision Processes  \n   Description: In-depth study of the Markov decision process, including the interactions between the agent and the environment.  \n   Learning Objectives: Understand and apply concepts of Markov properties and decision processes to calculate value functions.  \n\n   Subtopics:  \n   3.1 The Agent–Environment Interface  \n       Exploration of the interaction model between agents and their environments.  \n   3.2 Goals and Rewards  \n       Formulating goals and rewards in RL settings.  \n   3.3 Returns and Markov Properties  \n       Understanding Markov properties and their applicability.  \n   3.4 Markov Decision Process  \n       Comprehensive study of MDPs.  \n   3.5 Value Functions and Optimal Value Functions  \n       Developing and optimizing value functions.\n\n4. Dynamic Programming  \n   Description: Study of algorithms for solving RL problems through dynamic programming approaches.  \n   Learning Objectives: Master dynamic programming methods like policy evaluation and value iteration.  \n\n   Subtopics:  \n   4.1 Policy Evaluation (Prediction)  \n       Techniques for evaluating policies.  \n   4.2 Policy Improvement  \n       Methods of improving existing policies.  \n   4.3 Policy Iteration  \n       Process of iterative policy improvement.  \n   4.4 Value Iteration  \n       Strategies for calculating optimal values.  \n   4.5 Asynchronous Dynamic Programming  \n       Implementation of asynchronous approaches to dynamic programming.  \n   4.6 Generalized Policy Iteration  \n       Integrating multiple approaches for improved policy iteration.\n\n5. Monte Carlo Methods and Temporal-Difference Learning  \n   Description: Examination of advanced prediction and control methods in RL.  \n   Learning Objectives: Gain expertise in Monte Carlo methods and temporal-difference learning for better prediction and control.  \n   \n   Subtopics:  \n   5.1 Monte Carlo Prediction  \n       Implementing Monte Carlo methods for predicting future states.  \n   5.2 Monte Carlo Estimation of Action Values  \n       Using Monte Carlo methods to estimate the values of specific actions.  \n   5.3 Monte Carlo Control  \n       Strategies for controlling processes using Monte Carlo methods.  \n   5.4 TD Prediction  \n       Implementing temporal-difference learning for prediction.  \n   5.5 TD Control using Q-Learning  \n       Effective control in RL using Q-learning approaches.\n\n6. Applications and Case Studies  \n   Description: Exploration of practical applications of RL in various domains.  \n   Learning Objectives: Apply reinforcement learning principles to real-world situations and problems.  \n\n   Subtopics:  \n   6.1 Elevator Dispatching  \n       Implementing RL strategies in elevator dispatch systems.  \n   6.2 Dynamic Channel Allocation  \n       Application of RL in channel allocation for communication systems.  \n   6.3 Job-Shop Scheduling  \n       Using RL to enhance scheduling processes in job-shop environments.\n\nREFERENCES:  \n- Reinforcement Learning: An Introduction, by Richard S. Sutton and Andrew G. Barto  \n- Alessandro Palmas, Dr. Alexandra Galina Petre, Emanuele Ghelfi, The Reinforcement Learning Workshop: Learn how to Apply Cutting-edge Reinforcement Learning Algorithms to a Wide Range of Control Problems, 2020, Packt Publishing.  \n- Phil Winder, Reinforcement Learning Industrial Applications with Intelligent Agents, O’Reilly  \n- Dr Engr S M Farrukh Akhtar, Practical Reinforcement Learning, Packt Publishing, 2017."
  },
  "doc-1bf6476ae6424f36474aa78c431fce29": {
    "content": "EXAM DETAILS:\nYear: 2023-24\nSemester: VIII\nSubject: Reinforcement Learning\nTotal Marks: 20 (Mid Term Test), 60 (End Semester Examination)\nDuration: 1 hour (Mid Term Test), 2 hours (End Semester Examination)\n\nMID TERM TEST:\nClass: OE, AI & DS (D16AD) Division: A\nDate: 27th February, 2024\n\nQUESTIONS:\n---------------------\nQuestion 1: (Attempt any five of the following) (20 Marks)\n\nQ.1 a) Compare reinforcement learning with supervised and unsupervised learning. Explain the comparison criteria in short. (2 Marks)\nTopic: Fundamentals of Reinforcement Learning\nSubtopic: Comparison with other learning paradigms\nDifficulty: Medium\nCognitive Level: Comprehension\nQuestion Type: Short Answer\n\nQ.1 b) Differentiate between off-policy and on-policy learning in the context of reinforcement learning. (2 Marks)\nTopic: Reinforcement Learning Strategies\nSubtopic: Policy types\nDifficulty: Medium\nCognitive Level: Analysis\nQuestion Type: Short Answer\n\nQ.1 c) Discuss the significance of the discount factor in reinforcement learning. Elaborate on how the discount factor influences the agent's decision-making process and the trade-offs involved in selecting different values for the discount factor. (2 Marks)\nTopic: Fundamentals of Reinforcement Learning\nSubtopic: Discount factor\nDifficulty: Medium\nCognitive Level: Analysis\nQuestion Type: Short Answer\n\nQ.1 d) Explain the concept of Optimistic Initial Values in the context of reinforcement learning. Provide a brief description of how optimistic initial values influence the exploration-exploitation trade-off. (2 Marks)\nTopic: Exploration Strategies\nSubtopic: Initial value settings\nDifficulty: Medium\nCognitive Level: Comprehension\nQuestion Type: Short Answer\n\nQ.1 e) Explain how reinforcement learning can be applied to enhance the performance of a mobile robot tasked with collecting empty soda cans in an office environment. Briefly outline the key components or considerations in designing a reinforcement learning framework for this specific task. (2 Marks)\nTopic: Applications of Reinforcement Learning\nSubtopic: Robotics\nDifficulty: Medium\nCognitive Level: Application\nQuestion Type: Short Answer\n\nQ.1 f) Define the concepts of goals and rewards in the context of a Markov Decision Process (MDP). Additionally, briefly explain the terms returns and episodes in the context of reinforcement learning. (2 Marks)\nTopic: Fundamentals of Reinforcement Learning\nSubtopic: MDP\nDifficulty: Medium\nCognitive Level: Knowledge\nQuestion Type: Short Answer\n---------------------\nQuestion 2: (Select one)\n\nQ.2 a) Discuss the Upper-Confidence-Bound (UCB) Action Selection method in the context of multi-armed bandits. Provide a detailed explanation of how UCB balances exploration and exploitation in the decision-making process. Additionally, outline the key components involved in the UCB formula and how they contribute to the algorithm's effectiveness. Also discuss any potential challenges or considerations associated with the application of UCB. (5 Marks)\nTopic: Exploration Strategies\nSubtopic: Multi-armed bandits\nDifficulty: Hard\nCognitive Level: Analysis\nQuestion Type: Long Answer\n\nOR\n\nQ.2 b) Examine the Gradient Bandit Algorithms in the context of reinforcement learning. Discuss the fundamental principles underlying these algorithms, including the concept of softmax action selection and the role of preferences. Explain how the update rules in the algorithm adapt the preferences based on rewards and provide a comparison with other bandit algorithms, highlighting the advantages and potential limitations of Gradient Bandit Algorithms. Also discuss the impact of the learning rate on the algorithm's performance and convergence. (5 Marks)\nTopic: Advanced Reinforcement Learning Techniques\nSubtopic: Bandit algorithms\nDifficulty: Hard\nCognitive Level: Analysis\nQuestion Type: Long Answer\n---------------------\nQuestion 3: (Select one)\n\nQ.3 a) Consider an MDP with three states, denoted as A, B, and C, arranged in a loop. The transitions and actions are as follows: \nDiagram: [States A -> B -> C, C -> A. In each state, there are two possible actions: \"Moves\" and \"Stays.\" A reward of 1 is received when the agent takes the \"Moves\" action in state C. All other transitions result in a reward of 0. Assume the agent starts in state A, discount factor (γ) = 0.9 and learning rate (α) = 1]\nShow the Q values for 3 iterations using the Q-learning algorithm. (5 Marks)\nTopic: Reinforcement Learning Algorithms\nSubtopic: Q-learning\nDifficulty: Hard\nCognitive Level: Application\nQuestion Type: Problem-solving\n\nOR\n\nQ.3 b) Show the Q values for 3 iterations using the SARSA algorithm. (5 Marks)\nTopic: Reinforcement Learning Algorithms\nSubtopic: SARSA\nDifficulty: Hard\nCognitive Level: Application\nQuestion Type: Problem-solving\n\nEND SEMESTER EXAMINATION:\nMax marks: 60 Duration: 2 hours\n\nQUESTIONS:\n---------------------\nQ. l (a) Explain the Policy Improvement Theorem in the context of Reinforcement Learning. Describe the fundamental principle behind the theorem and its proof. Discuss the implications of the theorem on the iterative process of policy iteration. (10 Marks)\nTopic: Reinforcement Learning Theory\nSubtopic: Policy iteration\nDifficulty: Hard\nCognitive Level: Comprehension\nQuestion Type: Long Answer\n\nQ. l (b) Compare SARSA and Q-learning, highlighting the difference between on-policy and off-policy methods. Provide a suitable example. (10 Marks)\nTopic: Reinforcement Learning Algorithms\nSubtopic: SARSA vs Q-learning\nDifficulty: Medium\nCognitive Level: Analysis\nQuestion Type: Long Answer\n---------------------\nQ.2 (a) Differentiate between model-based and model-free types of Reinforcement Learning (RL). Discuss the advantages and limitations of each approach, providing real-world examples where each type would be most suitable. (10 Marks)\nTopic: Reinforcement Learning Paradigms\nSubtopic: Model-based vs Model-free RL\nDifficulty: Medium\nCognitive Level: Analysis\nQuestion Type: Long Answer\n\nQ.2 (b) Discuss the Iterative Policy Evaluation with the help of a suitable example. (10 Marks)\nTopic: Reinforcement Learning Techniques\nSubtopic: Policy evaluation\nDifficulty: Medium\nCognitive Level: Comprehension\nQuestion Type: Long Answer\n---------------------\nQ.3 (a) Explain the Markov properties and their role in constructing Markov Decision Processes (MDPs) in Reinforcement Learning. Formulate an MDP scenario depicting a bot collecting empty soda cans in an office environment as an illustration of how Markov properties are applied to model complex decision-making tasks. (10 Marks)\nTopic: Fundamentals of Reinforcement Learning\nSubtopic: Markov Decision Processes\nDifficulty: Hard\nCognitive Level: Application\nQuestion Type: Long Answer\n\nQ.3 (b) Explore Upper-Confidence-Bound (UCB) Action Selection in multi-armed bandits. Analyze UCB's formula and address potential application challenges. (10 Marks)\nTopic: Exploration Strategies\nSubtopic: UCB Action Selection\nDifficulty: Hard\nCognitive Level: Analysis\nQuestion Type: Long Answer\n---------------------\nQ.4 (a) Discuss the k-armed bandit problem, focusing on exploration-exploitation trade-offs. Discuss four practical applications of the k-armed bandit problem, across different domains, showcasing its adaptability in optimizing decision-making processes. (10 Marks)\nTopic: Exploration Strategies\nSubtopic: k-armed bandit problem\nDifficulty: Medium\nCognitive Level: Analysis\nQuestion Type: Long Answer\n\nQ.4 (b) Describe the concept of Monte Carlo Prediction in Reinforcement Learning. Write the pseudocode for first-visit Monte Carlo Prediction. Discuss the advantage of employing Monte Carlo methods over Dynamic Programming (DP) methods specifically in the context of the blackjack game. (10 Marks)\nTopic: Reinforcement Learning Algorithms\nSubtopic: Monte Carlo methods\nDifficulty: Medium\nCognitive Level: Application\nQuestion Type: Long Answer\n\nNote: The response includes a comprehensive analysis covering different difficulty levels, cognitive domains, and appropriate topics based on the provided question content."
  },
  "doc-31aa9c841d724e7a958338c15575adcb": {
    "content": "EXAM DETAILS:\nYear: 2024\nSemester: VIII\nSubject: Reinforcement Learning\nTotal Marks: 60\nDuration: 2 hours\n\nQUESTIONS:\n---------------------\nQuestion 1: \n(a) Explain the Policy Improvement Theorem in the context of Reinforcement Learning. Describe the fundamental principle behind the theorem and its proof. Discuss the implications of the theorem on the iterative process of policy iteration. [10 Marks]\n\n(b) Compare SARSA and Q-learning, highlighting the difference between on-policy and off-policy methods. Provide a suitable example. [10 Marks]\n\nTopic: Policy and Value Functions in Reinforcement Learning\nSubtopic: Policy Improvement Theorem and Comparison of Learning Methods\nMarks: 20\nDifficulty: Medium\nCognitive Level: Comprehension and Analysis\nQuestion Type: Long Answer\n---------------------\n\nQuestion 2:\n(a) Differentiate between model-based and model-free types of Reinforcement Learning (RL). Discuss the advantages and limitations of each approach, providing real-world examples where each type would be most suitable. [10 Marks]\n\n(b) Discuss the Iterative Policy Evaluation with the help' of a suitable example. [10 Marks]\n\nTopic: Types of Reinforcement Learning\nSubtopic: Model-Based vs Model-Free RL and Policy Evaluation\nMarks: 20\nDifficulty: Medium\nCognitive Level: Analysis and Application\nQuestion Type: Long Answer\n---------------------\n\nQuestion 3:\n(a) Explain the Markov properties and their role in constructing Markov Decision Processes (MDPs) in Reinforcement Learning. Formulate an MDP scenario depicting a bot collecting empty soda cans in an office environment as an illustration of how Markov properties are applied to model complex decision-making tasks. [10 Marks]\n\n(b) Explore Upper-Confidence-Bound (UCB) Action Selection in multi-armed bandits. Analyze UCB's formula and address potential application challenges. [10 Marks]\n\nTopic: Fundamental Concepts in Reinforcement Learning\nSubtopic: Markov Decision Processes and Action Selection Strategies\nMarks: 20\nDifficulty: Medium\nCognitive Level: Comprehension and Application\nQuestion Type: Long Answer\n---------------------\n\nQuestion 4:\n(a) Discuss the k-armed bandit problem, focusing on exploration-exploitation trade-offs. Discuss four practical applications of the k-armed bandit problem, across different domains, showcasing its adaptability in optimizing decision-making processes. [10 Marks]\n\n(b) Describe the concept of Monte Carlo Prediction in Reinforcement Learning. Write the pseudocode for first-visit Monte Carlo Prediction. Discuss the advantage of employing Monte Carlo methods over Dynamic Programming (DP) methods specifically in the context of the blackjack game. [10 Marks]\n\nTopic: Exploration-Exploitation in Reinforcement Learning\nSubtopic: Multi-Armed Bandits and Monte Carlo Methods\nMarks: 20\nDifficulty: Medium\nCognitive Level: Analysis and Application\nQuestion Type: Long Answer\n---------------------\n\nQuestion 5:\n(a) Design a Reinforcement Learning algorithm to optimize Dynamic Channel Allocation in a wireless communication network. Provide the state representation, action space, reward function, and exploration strategy. Discuss any one potential challenge in implementing such an algorithm in a real-world scenario. [10 Marks]\n\n(b) In the context of reinforcement learning evaluate the concepts of Goals, Rewards, Returns, Episodes and Discounting. Discuss the conventional representations and mathematical formulations associated with Goals, Rewards, Returns, Episodes and Discounting. [10 Marks]\n\nTopic: Applications and Theoretical Foundations in Reinforcement Learning\nSubtopic: Dynamic Allocation and Foundations of RL Concepts\nMarks: 20\nDifficulty: Hard\nCognitive Level: Design and Evaluation\nQuestion Type: Long Answer\n---------------------"
  }
}