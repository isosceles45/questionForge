var graphJson = {"directed": false, "multigraph": false, "graph": {"node_default": {}, "edge_default": {}}, "nodes": [{"entity_type": "SUBJECT", "description": "Reinforcement Learning is a field of artificial intelligence that focuses on training algorithms using a system of rewards and penalties. This discipline provides the means to design agents that make a sequence of decisions by learning from their interactions with an environment.<SEP>The study of how agents can learn to make decisions by interacting with their environment to maximize cumulative reward. It includes theories, algorithms, and applications, focusing on strategies for exploration and exploitation. Total Marks: 20 (Mid Term Test), 60 (End Semester Examination).<SEP>This course provides an in-depth exploration of reinforcement learning (RL) covering foundational concepts, algorithms, and diverse applications. The primary emphasis is on understanding rewards, decision-making processes through Markov decision processes, and the application of advanced RL algorithms to real-world problems.", "source_id": "chunk-31aa9c841d724e7a958338c15575adcb<SEP>chunk-5d32567366bb059b9b2b2aefd5dbd87f<SEP>chunk-5e54a0db75e608a382c1ce071f140ac3", "clusters": "[{level: 0, cluster: 7}, {level: 1, cluster: 27}]", "id": "REINFORCEMENT LEARNING"}, {"entity_type": "TOPIC", "description": "Basic knowledge of probability distributions, expected values, and fundamental linear algebra concepts such as inner products.", "source_id": "chunk-5e54a0db75e608a382c1ce071f140ac3", "clusters": "[{level: 0, cluster: 7}, {level: 1, cluster: 27}]", "id": "PREREQUISITE"}, {"entity_type": "TOPIC", "description": "A foundational overview of Reinforcement Learning, its key features, and elements, including types of RL and the role of rewards. Learning objectives include understanding core concepts of RL and differentiating between various algorithms.", "source_id": "chunk-5e54a0db75e608a382c1ce071f140ac3", "clusters": "[{level: 0, cluster: 7}, {level: 1, cluster: 27}]", "id": "INTRODUCTION TO REINFORCEMENT LEARNING"}, {"entity_type": "TOPIC", "description": "Detailed exploration of algorithms such as Q-learning and SARSA used for policy development and decision-making in reinforcement learning.<SEP>Includes Q-Learning and State Action Reward State Action (SARSA).<SEP>This topic encompasses various algorithms within reinforcement learning, focusing on their design, function, and comparative advantages.", "source_id": "chunk-312b45904d3587796a9473ebd66e30d1<SEP>chunk-5d32567366bb059b9b2b2aefd5dbd87f<SEP>chunk-5e54a0db75e608a382c1ce071f140ac3", "clusters": "[{level: 0, cluster: 7}, {level: 1, cluster: 27}]", "id": "REINFORCEMENT LEARNING ALGORITHMS"}, {"entity_type": "TOPIC", "description": "Introduction to bandit problems and methodologies used in online learning contexts. Learning Objectives: Develop the ability to solve n-Armed Bandit Problems and implement action-value methods.", "source_id": "chunk-5e54a0db75e608a382c1ce071f140ac3", "clusters": "[{level: 0, cluster: 6}, {level: 1, cluster: 23}]", "id": "BANDIT PROBLEMS AND ONLINE LEARNING"}, {"entity_type": "SUBTOPIC", "description": "Discuss approaches to solving this fundamental problem in decision making.", "source_id": "chunk-5e54a0db75e608a382c1ce071f140ac3", "clusters": "[{level: 0, cluster: 6}, {level: 1, cluster: 23}]", "id": "AN N-ARMED BANDIT PROBLEM"}, {"entity_type": "SUBTOPIC", "description": "Techniques for tracking nonstationary problems.", "source_id": "chunk-5e54a0db75e608a382c1ce071f140ac3", "clusters": "[{level: 0, cluster: 6}, {level: 1, cluster: 23}]", "id": "ACTION-VALUE METHODS"}, {"entity_type": "CONCEPT", "description": "Initial estimates used in reinforcement learning to encourage exploration during early stages of learning.<SEP>Strategies for effective action selection using optimistic initial values.", "source_id": "chunk-5d32567366bb059b9b2b2aefd5dbd87f<SEP>chunk-5e54a0db75e608a382c1ce071f140ac3", "clusters": "[{level: 0, cluster: 6}, {level: 1, cluster: 23}]", "id": "OPTIMISTIC INITIAL VALUES"}, {"entity_type": "SUBTOPIC", "description": "Learn about using confidence bounds to inform decision making.", "source_id": "chunk-5e54a0db75e608a382c1ce071f140ac3", "clusters": "[{level: 0, cluster: 6}, {level: 1, cluster: 23}]", "id": "UPPER-CONFIDENCE-BOUND ACTION SELECTION"}, {"entity_type": "SUBTOPIC", "description": "Implementing gradient-based approaches for bandit problems.", "source_id": "chunk-5e54a0db75e608a382c1ce071f140ac3", "clusters": "[{level: 0, cluster: 6}, {level: 1, cluster: 23}]", "id": "GRADIENT BANDITS"}, {"entity_type": "SUBTOPIC", "description": "In-depth study of the Markov decision process, including interactions between the agent and the environment. Learning Objectives: Understand and apply concepts of Markov properties and decision processes to calculate value functions.<SEP>The construction and understanding of Markov properties and their application in decision-making tasks within reinforcement learning, with an emphasis on hard-level applications.", "source_id": "chunk-312b45904d3587796a9473ebd66e30d1<SEP>chunk-5e54a0db75e608a382c1ce071f140ac3", "clusters": "[{level: 0, cluster: 2}, {level: 1, cluster: 16}]", "id": "MARKOV DECISION PROCESSES"}, {"entity_type": "SUBTOPIC", "description": "Exploration of the interaction model between agents and their environments.", "source_id": "chunk-5e54a0db75e608a382c1ce071f140ac3", "clusters": "[{level: 0, cluster: 2}, {level: 1, cluster: 16}]", "id": "THE AGENT-ENVIRONMENT INTERFACE"}, {"entity_type": "SUBTOPIC", "description": "Formulating goals and rewards in RL settings.", "source_id": "chunk-5e54a0db75e608a382c1ce071f140ac3", "clusters": "[{level: 0, cluster: 2}, {level: 1, cluster: 16}]", "id": "GOALS AND REWARDS"}, {"entity_type": "SUBTOPIC", "description": "Understanding Markov properties and their applicability.", "source_id": "chunk-5e54a0db75e608a382c1ce071f140ac3", "clusters": "[{level: 0, cluster: 2}, {level: 1, cluster: 16}]", "id": "RETURNS AND MARKOV PROPERTIES"}, {"entity_type": "SUBTOPIC", "description": "Comprehensive study of MDPs.", "source_id": "chunk-5e54a0db75e608a382c1ce071f140ac3", "clusters": "[{level: 0, cluster: 2}, {level: 1, cluster: 16}]", "id": "MARKOV DECISION PROCESS"}, {"entity_type": "SUBTOPIC", "description": "Developing and optimizing value functions.", "source_id": "chunk-5e54a0db75e608a382c1ce071f140ac3", "clusters": "[{level: 0, cluster: 2}, {level: 1, cluster: 16}]", "id": "VALUE FUNCTIONS AND OPTIMAL VALUE FUNCTIONS"}, {"entity_type": "TOPIC", "description": "Study of algorithms for solving RL problems through dynamic programming approaches. Learning Objectives: Master dynamic programming methods like policy evaluation and value iteration.", "source_id": "chunk-5e54a0db75e608a382c1ce071f140ac3", "clusters": "[{level: 0, cluster: 5}, {level: 1, cluster: 21}]", "id": "DYNAMIC PROGRAMMING"}, {"entity_type": "SUBTOPIC", "description": "Techniques for evaluating policies.", "source_id": "chunk-5e54a0db75e608a382c1ce071f140ac3", "clusters": "[{level: 0, cluster: 5}, {level: 1, cluster: 21}]", "id": "POLICY EVALUATION (PREDICTION)"}, {"entity_type": "SUBTOPIC", "description": "Methods of improving existing policies.", "source_id": "chunk-5e54a0db75e608a382c1ce071f140ac3", "clusters": "[{level: 0, cluster: 5}, {level: 1, cluster: 21}]", "id": "POLICY IMPROVEMENT"}, {"entity_type": "SUBTOPIC", "description": "A detailed exploration into the method of policy iteration within reinforcement learning, focusing on comprehension level understanding of its principles and application.<SEP>Analysis of iterative process for improving policies based on the Policy Improvement Theorem, involving proofs.<SEP>Process of iterative policy improvement.", "source_id": "chunk-312b45904d3587796a9473ebd66e30d1<SEP>chunk-5d32567366bb059b9b2b2aefd5dbd87f<SEP>chunk-5e54a0db75e608a382c1ce071f140ac3", "clusters": "[{level: 0, cluster: 5}, {level: 1, cluster: 22}]", "id": "POLICY ITERATION"}, {"entity_type": "SUBTOPIC", "description": "Strategies for calculating optimal values.", "source_id": "chunk-5e54a0db75e608a382c1ce071f140ac3", "clusters": "[{level: 0, cluster: 5}, {level: 1, cluster: 21}]", "id": "VALUE ITERATION"}, {"entity_type": "SUBTOPIC", "description": "Implementation of asynchronous approaches to dynamic programming.", "source_id": "chunk-5e54a0db75e608a382c1ce071f140ac3", "clusters": "[{level: 0, cluster: 5}, {level: 1, cluster: 21}]", "id": "ASYNCHRONOUS DYNAMIC PROGRAMMING"}, {"entity_type": "SUBTOPIC", "description": "Integrating multiple approaches for improved policy iteration.", "source_id": "chunk-5e54a0db75e608a382c1ce071f140ac3", "clusters": "[{level: 0, cluster: 5}, {level: 1, cluster: 21}]", "id": "GENERALIZED POLICY ITERATION"}, {"entity_type": "TOPIC", "description": "Examination of advanced prediction and control methods in RL. Learning Objectives: Gain expertise in Monte Carlo methods and temporal-difference learning for better prediction and control.", "source_id": "chunk-5e54a0db75e608a382c1ce071f140ac3", "clusters": "[{level: 0, cluster: 9}]", "id": "MONTE CARLO METHODS AND TEMPORAL-DIFFERENCE LEARNING"}, {"entity_type": "SUBTOPIC", "description": "Implementing Monte Carlo methods for predicting future states.<SEP>Monte Carlo Prediction involves estimating the value function of states or state-action pairs by averaging the returns received after visiting those states in episode-based models of learning.", "source_id": "chunk-31aa9c841d724e7a958338c15575adcb<SEP>chunk-5e54a0db75e608a382c1ce071f140ac3", "clusters": "[{level: 0, cluster: 9}]", "id": "MONTE CARLO PREDICTION"}, {"entity_type": "SUBTOPIC", "description": "Using Monte Carlo methods to estimate the values of specific actions.", "source_id": "chunk-5e54a0db75e608a382c1ce071f140ac3", "clusters": "[{level: 0, cluster: 9}]", "id": "MONTE CARLO ESTIMATION OF ACTION VALUES"}, {"entity_type": "SUBTOPIC", "description": "Strategies for controlling processes using Monte Carlo methods.", "source_id": "chunk-5e54a0db75e608a382c1ce071f140ac3", "clusters": "[{level: 0, cluster: 9}]", "id": "MONTE CARLO CONTROL"}, {"entity_type": "SUBTOPIC", "description": "Implementing temporal-difference learning for prediction.", "source_id": "chunk-5e54a0db75e608a382c1ce071f140ac3", "clusters": "[{level: 0, cluster: 9}]", "id": "TD PREDICTION"}, {"entity_type": "SUBTOPIC", "description": "Effective control in RL using Q-learning approaches.", "source_id": "chunk-5e54a0db75e608a382c1ce071f140ac3", "clusters": "[{level: 0, cluster: 9}]", "id": "TD CONTROL USING Q-LEARNING"}, {"entity_type": "TOPIC", "description": "Exploration of practical applications of RL in various domains. Learning Objectives: Apply reinforcement learning principles to real-world situations and problems.", "source_id": "chunk-5e54a0db75e608a382c1ce071f140ac3", "clusters": "[{level: 0, cluster: 3}]", "id": "APPLICATIONS AND CASE STUDIES"}, {"entity_type": "SUBTOPIC", "description": "Implementing RL strategies in elevator dispatch systems.", "source_id": "chunk-5e54a0db75e608a382c1ce071f140ac3", "clusters": "[{level: 0, cluster: 3}]", "id": "ELEVATOR DISPATCHING"}, {"entity_type": "SUBTOPIC", "description": "Application of RL in channel allocation for communication systems.<SEP>Dynamic Channel Allocation refers to the use of algorithms to manage and distribute wireless network channels efficiently, optimizing usage and minimizing interference based on real-time conditions.", "source_id": "chunk-31aa9c841d724e7a958338c15575adcb<SEP>chunk-5e54a0db75e608a382c1ce071f140ac3", "clusters": "[{level: 0, cluster: 3}]", "id": "DYNAMIC CHANNEL ALLOCATION"}, {"entity_type": "SUBTOPIC", "description": "Using RL to enhance scheduling processes in job-shop environments.", "source_id": "chunk-5e54a0db75e608a382c1ce071f140ac3", "clusters": "[{level: 0, cluster: 3}]", "id": "JOB-SHOP SCHEDULING"}, {"entity_type": "TOPIC", "description": "Includes multiple sources that provide supplemental information and details regarding techniques and applications of reinforcement learning.", "source_id": "chunk-5e54a0db75e608a382c1ce071f140ac3", "clusters": "[{level: 0, cluster: 10}]", "id": "REFERENCE MATERIALS"}, {"entity_type": "TOPIC", "description": "A model-free reinforcement learning algorithm used to find the optimal action-selection policy for any given finite Markov decision process.<SEP>Q-learning exploration involves addressing Q-value convergence issues with full exploration of states and actions.<SEP>Q-learning is an off-policy Reinforcement Learning algorithm that aims to find the best action to take given the current state, regardless of the action taken in the next state.", "source_id": "chunk-31aa9c841d724e7a958338c15575adcb<SEP>chunk-5d32567366bb059b9b2b2aefd5dbd87f<SEP>chunk-5e54a0db75e608a382c1ce071f140ac3", "clusters": "[{level: 0, cluster: 7}, {level: 1, cluster: 30}]", "id": "Q-LEARNING"}, {"entity_type": "SUBTOPIC", "description": "A reinforcement learning algorithm for learning Markov decision processes policies, which is an on-policy temporal difference learning method.", "source_id": "chunk-5e54a0db75e608a382c1ce071f140ac3", "id": "STATE ACTION REWARD STATE ACTION (SARSA)"}, {"entity_type": "COGNITIVE_LEVEL", "description": "Understand core concepts of Reinforcement Learning and differentiate between various RL algorithms.", "source_id": "chunk-5e54a0db75e608a382c1ce071f140ac3", "id": "LEARNING OBJECTIVES FOR INTRODUCTION TO REINFORCEMENT LEARNING"}, {"entity_type": "COGNITIVE_LEVEL", "description": "Develop the ability to solve n-Armed Bandit Problems and implement action-value methods.", "source_id": "chunk-5e54a0db75e608a382c1ce071f140ac3", "id": "LEARNING OBJECTIVES FOR BANDIT PROBLEMS AND ONLINE LEARNING"}, {"entity_type": "COGNITIVE_LEVEL", "description": "Understand and apply concepts of Markov properties and decision processes to calculate value functions.", "source_id": "chunk-5e54a0db75e608a382c1ce071f140ac3", "id": "LEARNING OBJECTIVES FOR MARKOV DECISION PROCESSES"}, {"entity_type": "COGNITIVE_LEVEL", "description": "Master dynamic programming methods like policy evaluation and value iteration.", "source_id": "chunk-5e54a0db75e608a382c1ce071f140ac3", "id": "LEARNING OBJECTIVES FOR DYNAMIC PROGRAMMING"}, {"entity_type": "COGNITIVE_LEVEL", "description": "Gain expertise in Monte Carlo methods and temporal-difference learning for better prediction and control.", "source_id": "chunk-5e54a0db75e608a382c1ce071f140ac3", "id": "LEARNING OBJECTIVES FOR MONTE CARLO METHODS AND TEMPORAL-DIFFERENCE LEARNING"}, {"entity_type": "COGNITIVE_LEVEL", "description": "Apply reinforcement learning principles to real-world situations and problems.", "source_id": "chunk-5e54a0db75e608a382c1ce071f140ac3", "id": "LEARNING OBJECTIVES FOR APPLICATIONS AND CASE STUDIES"}, {"source_id": "chunk-5e54a0db75e608a382c1ce071f140ac3", "description": "is referenced by", "entity_type": "UNKNOWN", "clusters": "[{level: 0, cluster: 10}]", "id": "REINFORCEMENT LEARNING: AN INTRODUCTION"}, {"source_id": "chunk-5e54a0db75e608a382c1ce071f140ac3", "description": "is referenced by", "entity_type": "UNKNOWN", "clusters": "[{level: 0, cluster: 10}]", "id": "THE REINFORCEMENT LEARNING WORKSHOP"}, {"source_id": "chunk-5e54a0db75e608a382c1ce071f140ac3", "description": "is referenced by", "entity_type": "UNKNOWN", "clusters": "[{level: 0, cluster: 10}]", "id": "REINFORCEMENT LEARNING INDUSTRIAL APPLICATIONS"}, {"source_id": "chunk-5e54a0db75e608a382c1ce071f140ac3", "description": "is referenced by", "entity_type": "UNKNOWN", "clusters": "[{level: 0, cluster: 10}]", "id": "PRACTICAL REINFORCEMENT LEARNING"}, {"entity_type": "TOPIC", "description": "Basic principles and concepts of Reinforcement Learning including comparisons with other learning paradigms and essential elements such as the discount factor and MDP.<SEP>Introduces basic concepts and principles underlying reinforcement learning, including the use of Markov Decision Processes for decision-making.", "source_id": "chunk-312b45904d3587796a9473ebd66e30d1<SEP>chunk-5d32567366bb059b9b2b2aefd5dbd87f", "clusters": "[{level: 0, cluster: 4}, {level: 1, cluster: 19}]", "id": "FUNDAMENTALS OF REINFORCEMENT LEARNING"}, {"entity_type": "SUBTOPIC", "description": "Examines the differences and similarities between reinforcement learning, supervised learning, and unsupervised learning.", "source_id": "chunk-5d32567366bb059b9b2b2aefd5dbd87f", "clusters": "[{level: 0, cluster: 4}, {level: 1, cluster: 19}]", "id": "COMPARISON WITH OTHER LEARNING PARADIGMS"}, {"entity_type": "SUBTOPIC", "description": "Covers concepts related to Markov Decision Processes including goals, rewards, returns, episodes, and discount factors.", "source_id": "chunk-5d32567366bb059b9b2b2aefd5dbd87f", "clusters": "[{level: 0, cluster: 4}, {level: 1, cluster: 18}]", "id": "MDP"}, {"entity_type": "TOPIC", "description": "Examination of various strategies employed in reinforcement learning to optimize policy learning and decision-making.", "source_id": "chunk-5d32567366bb059b9b2b2aefd5dbd87f", "clusters": "[{level: 0, cluster: 7}, {level: 1, cluster: 31}]", "id": "REINFORCEMENT LEARNING STRATEGIES"}, {"entity_type": "SUBTOPIC", "description": "Exploration of the distinctions between off-policy and on-policy learning methods in reinforcement learning.", "source_id": "chunk-5d32567366bb059b9b2b2aefd5dbd87f", "clusters": "[{level: 0, cluster: 7}, {level: 1, cluster: 31}]", "id": "POLICY TYPES"}, {"entity_type": "TOPIC", "description": "A focus on strategies for exploring reinforcement learning environments, including concepts like UCB action selection and k-armed bandit problems.<SEP>Study of techniques used to balance exploration and exploitation, ensuring complete environment understanding.", "source_id": "chunk-312b45904d3587796a9473ebd66e30d1<SEP>chunk-5d32567366bb059b9b2b2aefd5dbd87f", "clusters": "[{level: 0, cluster: 6}, {level: 1, cluster: 24}]", "id": "EXPLORATION STRATEGIES"}, {"entity_type": "SUBTOPIC", "description": "Discussion on the impact of initial value settings, particularly optimistic initial values, in the exploration-exploitation trade-off.", "source_id": "chunk-5d32567366bb059b9b2b2aefd5dbd87f", "clusters": "[{level: 0, cluster: 6}, {level: 1, cluster: 24}]", "id": "INITIAL VALUE SETTINGS"}, {"entity_type": "SUBTOPIC", "description": "Focus on the Upper-Confidence-Bound method for selecting actions in multi-armed bandit problems to balance exploration and exploitation.<SEP>Investigates reinforcement learning exploration strategies, focusing on decision-making across uncertain options, as exemplified by UCB Action Selection.", "source_id": "chunk-312b45904d3587796a9473ebd66e30d1<SEP>chunk-5d32567366bb059b9b2b2aefd5dbd87f", "clusters": "[{level: 0, cluster: 6}, {level: 1, cluster: 25}]", "id": "MULTI-ARMED BANDITS"}, {"entity_type": "TOPIC", "description": "Analysis of practical uses of reinforcement learning including its application in robotics, particularly in enhancing task performance of mobile robots.", "source_id": "chunk-5d32567366bb059b9b2b2aefd5dbd87f", "clusters": "[{level: 0, cluster: 7}, {level: 1, cluster: 29}]", "id": "APPLICATIONS OF REINFORCEMENT LEARNING"}, {"entity_type": "SUBTOPIC", "description": "Investigates how reinforcement learning is applied to improve robots\' performance in specific tasks such as collecting soda cans.", "source_id": "chunk-5d32567366bb059b9b2b2aefd5dbd87f", "clusters": "[{level: 0, cluster: 7}, {level: 1, cluster: 29}]", "id": "ROBOTICS"}, {"entity_type": "TOPIC", "description": "Advanced algorithms and methods for efficient learning in complex environments including bandit algorithms.", "source_id": "chunk-5d32567366bb059b9b2b2aefd5dbd87f", "clusters": "[{level: 0, cluster: 0}]", "id": "ADVANCED REINFORCEMENT LEARNING TECHNIQUES"}, {"entity_type": "SUBTOPIC", "description": "Advanced analysis of algorithms for decision-making in uncertain environments, focusing on gradient bandit algorithms and softmax action selection.", "source_id": "chunk-5d32567366bb059b9b2b2aefd5dbd87f", "clusters": "[{level: 0, cluster: 0}]", "id": "BANDIT ALGORITHMS"}, {"entity_type": "TOPIC", "description": "SARSA is an on-policy Reinforcement Learning algorithm that stands for State-Action-Reward-State-Action, used for learning the optimal policy by considering the action taken in the current state.<SEP>Study of the on-policy learning algorithm SARSA that evaluates the selected actions directly for policy improvement.", "source_id": "chunk-31aa9c841d724e7a958338c15575adcb<SEP>chunk-5d32567366bb059b9b2b2aefd5dbd87f", "clusters": "[{level: 0, cluster: 7}, {level: 1, cluster: 28}]", "id": "SARSA"}, {"entity_type": "TOPIC", "description": "Theoretical underpinnings of reinforcement learning focusing on policy improvement and iteration to enhance performance.<SEP>This topic covers foundational aspects of reinforcement learning, including policy iteration, with learning objectives aimed at understanding complex theories and strategies.", "source_id": "chunk-312b45904d3587796a9473ebd66e30d1<SEP>chunk-5d32567366bb059b9b2b2aefd5dbd87f", "clusters": "[{level: 0, cluster: 5}, {level: 1, cluster: 22}]", "id": "REINFORCEMENT LEARNING THEORY"}, {"entity_type": "QUESTION", "description": "Compare reinforcement learning with supervised and unsupervised learning. Explain the comparison criteria briefly. Topic: Fundamentals of Reinforcement Learning, Subtopic: Comparison with other learning paradigms, Difficulty: Medium, Cognitive Level: Comprehension, Marks: 2.", "source_id": "chunk-5d32567366bb059b9b2b2aefd5dbd87f", "clusters": "[{level: 0, cluster: 4}, {level: 1, cluster: 19}]", "id": "QUESTION 1A"}, {"entity_type": "QUESTION", "description": "Differentiate between off-policy and on-policy learning in the context of reinforcement learning. Topic: Reinforcement Learning Strategies, Subtopic: Policy types, Difficulty: Medium, Cognitive Level: Analysis, Marks: 2.", "source_id": "chunk-5d32567366bb059b9b2b2aefd5dbd87f", "clusters": "[{level: 0, cluster: 7}, {level: 1, cluster: 31}]", "id": "QUESTION 1B"}, {"entity_type": "QUESTION", "description": "Discuss the significance of the discount factor in reinforcement learning and its influence on the agent\'s decision-making process. Topic: Fundamentals of Reinforcement Learning, Subtopic: Discount factor, Difficulty: Medium, Cognitive Level: Analysis, Marks: 2.", "source_id": "chunk-5d32567366bb059b9b2b2aefd5dbd87f", "clusters": "[{level: 0, cluster: 4}, {level: 1, cluster: 19}]", "id": "QUESTION 1C"}, {"entity_type": "QUESTION", "description": "Explain the concept of Optimistic Initial Values and its effect on exploration-exploitation trade-off. Topic: Exploration Strategies, Subtopic: Initial value settings, Difficulty: Medium, Cognitive Level: Comprehension, Marks: 2.", "source_id": "chunk-5d32567366bb059b9b2b2aefd5dbd87f", "clusters": "[{level: 0, cluster: 6}, {level: 1, cluster: 24}]", "id": "QUESTION 1D"}, {"entity_type": "QUESTION", "description": "Explain RL application for mobile robots in collecting empty soda cans, outlining key design components. Topic: Applications of Reinforcement Learning, Subtopic: Robotics, Difficulty: Medium, Cognitive Level: Application, Marks: 2.", "source_id": "chunk-5d32567366bb059b9b2b2aefd5dbd87f", "clusters": "[{level: 0, cluster: 7}, {level: 1, cluster: 29}]", "id": "QUESTION 1E"}, {"entity_type": "QUESTION", "description": "Define goals and rewards in MDP; explain returns and episodes in reinforcement learning. Topic: Fundamentals of Reinforcement Learning, Subtopic: MDP, Difficulty: Medium, Cognitive Level: Knowledge, Marks: 2.", "source_id": "chunk-5d32567366bb059b9b2b2aefd5dbd87f", "clusters": "[{level: 0, cluster: 4}, {level: 1, cluster: 18}]", "id": "QUESTION 1F"}, {"entity_type": "QUESTION", "description": "Discuss UCB Action Selection in multi-armed bandits, including key components and challenges. Topic: Exploration Strategies, Subtopic: Multi-armed bandits, Difficulty: Hard, Cognitive Level: Analysis, Marks: 5.", "source_id": "chunk-5d32567366bb059b9b2b2aefd5dbd87f", "clusters": "[{level: 0, cluster: 6}, {level: 1, cluster: 25}]", "id": "QUESTION 2A"}, {"entity_type": "QUESTION", "description": "Examine Gradient Bandit Algorithms, softmax action selection, and the impact of the learning rate. Topic: Advanced Reinforcement Learning Techniques, Subtopic: Bandit algorithms, Difficulty: Hard, Cognitive Level: Analysis, Marks: 5.", "source_id": "chunk-5d32567366bb059b9b2b2aefd5dbd87f", "clusters": "[{level: 0, cluster: 0}]", "id": "QUESTION 2B"}, {"entity_type": "QUESTION", "description": "Show Q values for 3 iterations using Q-learning. Topic: Reinforcement Learning Algorithms, Subtopic: Q-learning, Difficulty: Hard, Cognitive Level: Application, Marks: 5.", "source_id": "chunk-5d32567366bb059b9b2b2aefd5dbd87f", "clusters": "[{level: 0, cluster: 7}, {level: 1, cluster: 30}]", "id": "QUESTION 3A"}, {"entity_type": "QUESTION", "description": "Show Q values for 3 iterations using SARSA. Topic: Reinforcement Learning Algorithms, Subtopic: SARSA, Difficulty: Hard, Cognitive Level: Application, Marks: 5.", "source_id": "chunk-5d32567366bb059b9b2b2aefd5dbd87f", "clusters": "[{level: 0, cluster: 7}, {level: 1, cluster: 28}]", "id": "QUESTION 3B"}, {"entity_type": "QUESTION", "description": "Explain the Policy Improvement Theorem with its implications on policy iteration. Topic: Reinforcement Learning Theory, Subtopic: Policy iteration, Difficulty: Hard, Cognitive Level: Comprehension, Marks: 10.", "source_id": "chunk-5d32567366bb059b9b2b2aefd5dbd87f", "clusters": "[{level: 0, cluster: 5}, {level: 1, cluster: 22}]", "id": "QUESTION LA"}, {"entity_type": "QUESTION", "description": "Compare SARSA and Q-learning in terms of on-policy and off-policy methods, providing examples. Topic: Reinforcement Learning Algorithms, Subtopic: SARSA vs Q-learning, Difficulty: Medium, Cognitive Level: Analysis, Marks: 10.", "source_id": "chunk-5d32567366bb059b9b2b2aefd5dbd87f", "clusters": "[{level: 0, cluster: 1}, {level: 1, cluster: 14}]", "id": "QUESTION LB"}, {"entity_type": "SUBTOPIC", "description": "Essential element in reinforcement learning, influencing decision-making and balancing immediate versus future rewards.", "source_id": "chunk-5d32567366bb059b9b2b2aefd5dbd87f", "clusters": "[{level: 0, cluster: 4}, {level: 1, cluster: 19}]", "id": "DISCOUNT FACTOR"}, {"entity_type": "CONCEPT", "description": "Defined objectives that an agent aims to achieve within the framework of a Markov Decision Process (MDP).", "source_id": "chunk-5d32567366bb059b9b2b2aefd5dbd87f", "clusters": "[{level: 0, cluster: 4}, {level: 1, cluster: 18}]", "id": "GOALS"}, {"entity_type": "CONCEPT", "description": "Feedback provided to an agent in reinforcement learning, guiding the learning process by indicating success or failure of actions.", "source_id": "chunk-5d32567366bb059b9b2b2aefd5dbd87f", "clusters": "[{level: 0, cluster: 4}, {level: 1, cluster: 18}]", "id": "REWARDS"}, {"entity_type": "CONCEPT", "description": "Cumulative reward an agent receives, often discounted over time in reinforcement learning contexts like MDPs.", "source_id": "chunk-5d32567366bb059b9b2b2aefd5dbd87f", "clusters": "[{level: 0, cluster: 4}, {level: 1, cluster: 18}]", "id": "RETURNS"}, {"entity_type": "CONCEPT", "description": "Sequences of states, actions, and rewards in reinforcement learning, representing one complete trajectory through the environment.", "source_id": "chunk-5d32567366bb059b9b2b2aefd5dbd87f", "clusters": "[{level: 0, cluster: 4}, {level: 1, cluster: 18}]", "id": "EPISODES"}, {"entity_type": "DIFFICULTY_LEVEL", "description": "Classification of cognitive tasks required for questions, ranging from knowledge and comprehension to application and analysis.", "source_id": "chunk-5d32567366bb059b9b2b2aefd5dbd87f", "id": "COGNITIVE LEVEL"}, {"entity_type": "CONCEPT", "description": "Challenge in reinforcement learning to find an optimal balance between exploring new actions and exploiting known rewards.", "source_id": "chunk-5d32567366bb059b9b2b2aefd5dbd87f", "id": "EXPLORATION-EXPLOITATION TRADE-OFF"}, {"entity_type": "CONCEPT", "description": "An action selection strategy in multi-armed bandits optimizing exploration-exploitation balance by setting confidence intervals.", "source_id": "chunk-5d32567366bb059b9b2b2aefd5dbd87f", "clusters": "[{level: 0, cluster: 6}, {level: 1, cluster: 25}]", "id": "UPPER-CONFIDENCE-BOUND (UCB)"}, {"entity_type": "CONCEPT", "description": "A class of algorithms using preferences to select actions, improving progressively by encouraging rewarding actions more.", "source_id": "chunk-5d32567366bb059b9b2b2aefd5dbd87f", "clusters": "[{level: 0, cluster: 0}]", "id": "GRADIENT BANDIT ALGORITHMS"}, {"entity_type": "CONCEPT", "description": "Technique in reinforcement learning for probabilistic selection of actions based on preference distribution.", "source_id": "chunk-5d32567366bb059b9b2b2aefd5dbd87f", "clusters": "[{level: 0, cluster: 0}]", "id": "SOFTMAX ACTION SELECTION"}, {"entity_type": "CONCEPT", "description": "Parameter in algorithms dictating the degree to which new information impacts current knowledge or preferences.", "source_id": "chunk-5d32567366bb059b9b2b2aefd5dbd87f", "clusters": "[{level: 0, cluster: 0}]", "id": "LEARNING RATE"}, {"entity_type": "CONCEPT", "description": "A theorem central to policy iteration in reinforcement learning, aiding in refining decision policies for better outcomes.", "source_id": "chunk-5d32567366bb059b9b2b2aefd5dbd87f", "clusters": "[{level: 0, cluster: 5}, {level: 1, cluster: 22}]", "id": "POLICY IMPROVEMENT THEOREM"}, {"entity_type": "CONCEPT", "description": "Learning approach where an agent improves its policy based on actions it actively selects and follows.", "source_id": "chunk-5d32567366bb059b9b2b2aefd5dbd87f", "clusters": "[{level: 0, cluster: 7}, {level: 1, cluster: 28}]", "id": "ON-POLICY LEARNING"}, {"entity_type": "CONCEPT", "description": "Learning framework where policy learning is based on exploring actions from outside or simulated source experiences.", "source_id": "chunk-5d32567366bb059b9b2b2aefd5dbd87f", "clusters": "[{level: 0, cluster: 7}, {level: 1, cluster: 30}]", "id": "OFF-POLICY LEARNING"}, {"entity_type": "SUBTOPIC", "description": "An analysis of SARSA and Q-learning algorithms under reinforcement learning, highlighting the differences between on-policy and off-policy methods.", "source_id": "chunk-312b45904d3587796a9473ebd66e30d1", "clusters": "[{level: 0, cluster: 1}, {level: 1, cluster: 14}]", "id": "SARSA VS Q-LEARNING"}, {"entity_type": "QUESTION", "description": "A medium difficulty question requiring analysis of SARSA and Q-learning, comparing their methods and applications.", "source_id": "chunk-312b45904d3587796a9473ebd66e30d1", "clusters": "[{level: 0, cluster: 1}, {level: 1, cluster: 14}]", "id": "COMPARE SARSA AND Q-LEARNING, HIGHLIGHTING THE DIFFERENCE BETWEEN ON-POLICY AND OFF-POLICY METHODS. PROVIDE A SUITABLE EXAMPLE. (10 MARKS)"}, {"entity_type": "SUBTOPIC", "description": "An examination of the types of reinforcement learning paradigms, focusing on model-based and model-free approaches, their advantages, limitations, and suitable real-world examples.", "source_id": "chunk-312b45904d3587796a9473ebd66e30d1", "clusters": "[{level: 0, cluster: 1}, {level: 1, cluster: 13}]", "id": "MODEL-BASED VS MODEL-FREE RL"}, {"entity_type": "QUESTION", "description": "A medium difficulty long-answer question requiring analysis on RL paradigms, focusing on understanding and application in real-world contexts.", "source_id": "chunk-312b45904d3587796a9473ebd66e30d1", "clusters": "[{level: 0, cluster: 1}, {level: 1, cluster: 13}]", "id": "DIFFERENTIATE BETWEEN MODEL-BASED AND MODEL-FREE TYPES OF REINFORCEMENT LEARNING (RL). DISCUSS THE ADVANTAGES AND LIMITATIONS OF EACH APPROACH, PROVIDING REAL-WORLD EXAMPLES WHERE EACH TYPE WOULD BE MOST SUITABLE. (10 MARKS)"}, {"entity_type": "SUBTOPIC", "description": "Delving into techniques for evaluating policies within reinforcement learning, emphasizing comprehension of iterative methods.", "source_id": "chunk-312b45904d3587796a9473ebd66e30d1", "clusters": "[{level: 0, cluster: 1}, {level: 1, cluster: 15}]", "id": "POLICY EVALUATION"}, {"entity_type": "QUESTION", "description": "A medium difficulty question focusing on comprehension of iterative policy evaluation in reinforcement learning, requiring illustrative examples.", "source_id": "chunk-312b45904d3587796a9473ebd66e30d1", "clusters": "[{level: 0, cluster: 1}, {level: 1, cluster: 15}]", "id": "DISCUSS THE ITERATIVE POLICY EVALUATION WITH THE HELP OF A SUITABLE EXAMPLE. (10 MARKS)"}, {"entity_type": "QUESTION", "description": "A hard difficulty application question examining Markov properties and their usage in MDPs within a practical scenario in reinforcement learning.", "source_id": "chunk-312b45904d3587796a9473ebd66e30d1", "clusters": "[{level: 0, cluster: 2}, {level: 1, cluster: 16}]", "id": "EXPLAIN THE MARKOV PROPERTIES AND THEIR ROLE IN CONSTRUCTING MARKOV DECISION PROCESSES (MDPS) IN REINFORCEMENT LEARNING. FORMULATE AN MDP SCENARIO DEPICTING A BOT COLLECTING EMPTY SODA CANS IN AN OFFICE ENVIRONMENT AS AN ILLUSTRATION OF HOW MARKOV PROPERTIES ARE APPLIED TO MODEL COMPLEX DECISION-MAKING TASKS. (10 MARKS)"}, {"entity_type": "SUBTOPIC", "description": "An analysis of the Upper-Confidence-Bound (UCB) method within exploration strategies in reinforcement learning, addressing its formula and challenges.", "source_id": "chunk-312b45904d3587796a9473ebd66e30d1", "clusters": "[{level: 0, cluster: 5}, {level: 1, cluster: 20}]", "id": "UCB ACTION SELECTION"}, {"entity_type": "QUESTION", "description": "A hard-level analysis question addressing Upper-Confidence-Bound Action Selection within multi-armed bandit settings in reinforcement learning.", "source_id": "chunk-312b45904d3587796a9473ebd66e30d1", "clusters": "[{level: 0, cluster: 5}, {level: 1, cluster: 20}]", "id": "EXPLORE UPPER-CONFIDENCE-BOUND (UCB) ACTION SELECTION IN MULTI-ARMED BANDITS. ANALYZE UCB\'S FORMULA AND ADDRESS POTENTIAL APPLICATION CHALLENGES. (10 MARKS)"}, {"entity_type": "TOPIC", "description": "Discussion about the exploration-exploitation trade-offs in the k-armed bandit problem, including practical applications across domains.<SEP>K-armed Bandit Problem is a problem in which an agent has to choose among K actions (or arms) repeatedly in order to maximize some notion of accumulated reward by efficiently exploring and exploiting different actions.", "source_id": "chunk-312b45904d3587796a9473ebd66e30d1<SEP>chunk-31aa9c841d724e7a958338c15575adcb", "clusters": "[{level: 0, cluster: 1}, {level: 1, cluster: 12}]", "id": "K-ARMED BANDIT PROBLEM"}, {"entity_type": "QUESTION", "description": "A medium difficulty question addressing exploration-exploitation trade-offs in k-armed bandit problems with practical applications.", "source_id": "chunk-312b45904d3587796a9473ebd66e30d1", "clusters": "[{level: 0, cluster: 1}, {level: 1, cluster: 12}]", "id": "DISCUSS THE K-ARMED BANDIT PROBLEM, FOCUSING ON EXPLORATION-EXPLOITATION TRADE-OFFS. DISCUSS FOUR PRACTICAL APPLICATIONS OF THE K-ARMED BANDIT PROBLEM, ACROSS DIFFERENT DOMAINS, SHOWCASING ITS ADAPTABILITY IN OPTIMIZING DECISION-MAKING PROCESSES. (10 MARKS)"}, {"entity_type": "SUBTOPIC", "description": "Describes Monte Carlo Prediction within reinforcement learning, its advantages over dynamic programming, and specific applications, such as in blackjack.", "source_id": "chunk-312b45904d3587796a9473ebd66e30d1", "clusters": "[{level: 0, cluster: 2}, {level: 1, cluster: 17}]", "id": "MONTE CARLO METHODS"}, {"entity_type": "QUESTION", "description": "A medium difficulty application question on Monte Carlo Prediction in reinforcement learning, including pseudocode and comparative analysis with DP methods.", "source_id": "chunk-312b45904d3587796a9473ebd66e30d1", "clusters": "[{level: 0, cluster: 2}, {level: 1, cluster: 17}]", "id": "DESCRIBE THE CONCEPT OF MONTE CARLO PREDICTION IN REINFORCEMENT LEARNING. WRITE THE PSEUDOCODE FOR FIRST-VISIT MONTE CARLO PREDICTION. DISCUSS THE ADVANTAGE OF EMPLOYING MONTE CARLO METHODS OVER DYNAMIC PROGRAMMING (DP) METHODS SPECIFICALLY IN THE CONTEXT OF THE BLACKJACK GAME. (10 MARKS)"}, {"entity_type": "DIFFICULTY_LEVEL", "description": "Represents the level of difficulty requiring an advanced understanding and application, as used in several questions.<SEP>This indicates a level where questions require higher-order thinking skills, such as designing and evaluating complex systems.", "source_id": "chunk-312b45904d3587796a9473ebd66e30d1<SEP>chunk-31aa9c841d724e7a958338c15575adcb", "clusters": "[{level: 0, cluster: 5}, {level: 1, cluster: 20}]", "id": "HARD"}, {"entity_type": "DIFFICULTY_LEVEL", "description": "Indicates a moderate level of challenge in questions requiring analysis and comprehension.<SEP>This indicates a difficulty level where the questions require moderate comprehension, analysis, and application.", "source_id": "chunk-312b45904d3587796a9473ebd66e30d1<SEP>chunk-31aa9c841d724e7a958338c15575adcb", "clusters": "[{level: 0, cluster: 1}, {level: 1, cluster: 15}]", "id": "MEDIUM"}, {"entity_type": "COGNITIVE_LEVEL", "description": "Denotes cognitive activities involving understanding and interpretation of concepts.", "source_id": "chunk-312b45904d3587796a9473ebd66e30d1", "clusters": "[{level: 0, cluster: 1}, {level: 1, cluster: 15}]", "id": "COMPREHENSION"}, {"entity_type": "COGNITIVE_LEVEL", "description": "Entails breaking down information and understanding relationships for in-depth conclusions.", "source_id": "chunk-312b45904d3587796a9473ebd66e30d1", "clusters": "[{level: 0, cluster: 1}, {level: 1, cluster: 12}]", "id": "ANALYSIS"}, {"entity_type": "COGNITIVE_LEVEL", "description": "Uses theoretical knowledge in practical scenarios to demonstrate understanding and usage.", "source_id": "chunk-312b45904d3587796a9473ebd66e30d1", "clusters": "[{level: 0, cluster: 2}, {level: 1, cluster: 17}]", "id": "APPLICATION"}, {"entity_type": "TOPIC", "description": "Explores different paradigms in reinforcement learning and their applications, focusing on model-based and model-free approaches.", "source_id": "chunk-312b45904d3587796a9473ebd66e30d1", "clusters": "[{level: 0, cluster: 1}, {level: 1, cluster: 13}]", "id": "REINFORCEMENT LEARNING PARADIGMS"}, {"entity_type": "TOPIC", "description": "Covers methods and strategies used in reinforcement learning to achieve optimal outcomes, including techniques like policy evaluation.", "source_id": "chunk-312b45904d3587796a9473ebd66e30d1", "clusters": "[{level: 0, cluster: 1}, {level: 1, cluster: 15}]", "id": "REINFORCEMENT LEARNING TECHNIQUES"}, {"entity_type": "SUBTOPIC", "description": "Analyzes the properties of Markov processes and how they contribute to forming Markov Decision Processes in reinforcement learning.", "source_id": "chunk-312b45904d3587796a9473ebd66e30d1", "id": "MARKOV PROPERTIES"}, {"entity_type": "QUESTION_PAPER", "description": "Defines the nature of questions in terms of format and expected responses, such as \'Long Answer\' types requiring detailed explanation.", "source_id": "chunk-312b45904d3587796a9473ebd66e30d1", "id": "QUESTION TYPE"}, {"entity_type": "QUESTION_TYPE", "description": "A type of question that demands a detailed and expansive response.", "source_id": "chunk-312b45904d3587796a9473ebd66e30d1", "id": "LONG ANSWER"}, {"entity_type": "TOPIC", "description": "A comparative exploration with Monte Carlo methods, focusing on problem-solving approaches in reinforcement learning.", "source_id": "chunk-312b45904d3587796a9473ebd66e30d1", "id": "DYNAMIC PROGRAMMING (DP) METHODS"}, {"entity_type": "TOPIC", "description": "A context used to illustrate the application of Monte Carlo methods in reinforcement learning, comparing their effectiveness to Dynamic Programming techniques.", "source_id": "chunk-312b45904d3587796a9473ebd66e30d1", "id": "BLACKJACK GAME"}, {"entity_type": "SUBTOPIC", "description": "A method in reinforcement learning for predicting returns based on the first appearance of each state.", "source_id": "chunk-312b45904d3587796a9473ebd66e30d1", "id": "FIRST-VISIT MONTE CARLO PREDICTION"}, {"entity_type": "ENTITY_TYPE", "description": "Goals set for understanding and application of reinforcement learning concepts and methods for students.", "source_id": "chunk-312b45904d3587796a9473ebd66e30d1", "id": "LEARNING OBJECTIVES"}, {"entity_type": "TOPIC", "description": "Policy and Value Functions are central concepts in Reinforcement Learning that help in determining optimal actions and estimating the values of policies.", "source_id": "chunk-31aa9c841d724e7a958338c15575adcb", "clusters": "[{level: 0, cluster: 7}, {level: 1, cluster: 27}]", "id": "POLICY AND VALUE FUNCTIONS IN REINFORCEMENT LEARNING"}, {"entity_type": "SUBTOPIC", "description": "This subtopic involves understanding the Policy Improvement Theorem, comparing on-policy (SARSA) and off-policy (Q-learning) methods, and analyzing their implications within Reinforcement Learning.", "source_id": "chunk-31aa9c841d724e7a958338c15575adcb", "clusters": "[{level: 0, cluster: 11}, {level: 1, cluster: 36}]", "id": "POLICY IMPROVEMENT THEOREM AND COMPARISON OF LEARNING METHODS"}, {"entity_type": "QUESTION", "description": "Explain the Policy Improvement Theorem in the context of Reinforcement Learning. Describe the fundamental principle behind the theorem and its proof. Discuss the implications of the theorem on the iterative process of policy iteration. [10 Marks]", "source_id": "chunk-31aa9c841d724e7a958338c15575adcb", "clusters": "[{level: 0, cluster: 11}, {level: 1, cluster: 36}]", "id": "QUESTION 1(A)"}, {"entity_type": "QUESTION", "description": "Compare SARSA and Q-learning, highlighting the difference between on-policy and off-policy methods. Provide a suitable example. [10 Marks]", "source_id": "chunk-31aa9c841d724e7a958338c15575adcb", "clusters": "[{level: 0, cluster: 11}, {level: 1, cluster: 36}]", "id": "QUESTION 1(B)"}, {"entity_type": "COGNITIVE_LEVEL", "description": "This cognitive level requires understanding and analyzing concepts to tackle the questions.", "source_id": "chunk-31aa9c841d724e7a958338c15575adcb", "id": "COMPREHENSION AND ANALYSIS"}, {"entity_type": "QUESTION_PAPER", "description": "A set of questions for the 2024 Semester VIII exam in Reinforcement Learning, designed to evaluate the breadth and depth of knowledge in the field over a 2-hour period with a total of 60 marks.<SEP>This question paper is for the 2024 Semester VIII examination on Reinforcement Learning, consisting of various questions that test different levels of understanding and application of the subject material, with a total of 60 marks and a duration of 2 hours.", "source_id": "chunk-31aa9c841d724e7a958338c15575adcb", "clusters": "[{level: 0, cluster: 11}, {level: 1, cluster: 32}]", "id": "QUESTION PAPER 2024 SEMESTER VIII"}, {"entity_type": "TOPIC", "description": "Exploration of different types of Reinforcement Learning methods, including model-based and model-free approaches, and the evaluation of policies.", "source_id": "chunk-31aa9c841d724e7a958338c15575adcb", "clusters": "[{level: 0, cluster: 8}]", "id": "TYPES OF REINFORCEMENT LEARNING"}, {"entity_type": "SUBTOPIC", "description": "This subtopic covers the distinction between model-based and model-free approaches in RL, including their benefits and limitations, alongside policy evaluation techniques.", "source_id": "chunk-31aa9c841d724e7a958338c15575adcb", "clusters": "[{level: 0, cluster: 11}, {level: 1, cluster: 35}]", "id": "MODEL-BASED VS MODEL-FREE RL AND POLICY EVALUATION"}, {"entity_type": "QUESTION", "description": "Differentiate between model-based and model-free types of Reinforcement Learning (RL). Discuss the advantages and limitations of each approach, providing real-world examples where each type would be most suitable. [10 Marks]", "source_id": "chunk-31aa9c841d724e7a958338c15575adcb", "clusters": "[{level: 0, cluster: 11}, {level: 1, cluster: 35}]", "id": "QUESTION 2(A)"}, {"entity_type": "QUESTION", "description": "Discuss the Iterative Policy Evaluation with the help of a suitable example. [10 Marks]", "source_id": "chunk-31aa9c841d724e7a958338c15575adcb", "clusters": "[{level: 0, cluster: 11}, {level: 1, cluster: 35}]", "id": "QUESTION 2(B)"}, {"entity_type": "TOPIC", "description": "The fundamental principles underpinning Reinforcement Learning, including Markov Decision Processes and strategies for action selection.", "source_id": "chunk-31aa9c841d724e7a958338c15575adcb", "clusters": "[{level: 0, cluster: 7}, {level: 1, cluster: 26}]", "id": "FUNDAMENTAL CONCEPTS IN REINFORCEMENT LEARNING"}, {"entity_type": "SUBTOPIC", "description": "Examining Markov Decision Processes, including the role of Markov properties and action selection strategies such as Upper-Confidence-Bound in multi-armed bandits.", "source_id": "chunk-31aa9c841d724e7a958338c15575adcb", "clusters": "[{level: 0, cluster: 11}, {level: 1, cluster: 34}]", "id": "MARKOV DECISION PROCESSES AND ACTION SELECTION STRATEGIES"}, {"entity_type": "QUESTION", "description": "Explain the Markov properties and their role in constructing Markov Decision Processes (MDPs) in Reinforcement Learning. Formulate an MDP scenario depicting a bot collecting empty soda cans in an office environment as an illustration of how Markov properties are applied to model complex decision-making tasks. [10 Marks]", "source_id": "chunk-31aa9c841d724e7a958338c15575adcb", "clusters": "[{level: 0, cluster: 11}, {level: 1, cluster: 34}]", "id": "QUESTION 3(A)"}, {"entity_type": "QUESTION", "description": "Explore Upper-Confidence-Bound (UCB) Action Selection in multi-armed bandits. Analyze UCB\'s formula and address potential application challenges. [10 Marks]", "source_id": "chunk-31aa9c841d724e7a958338c15575adcb", "clusters": "[{level: 0, cluster: 11}, {level: 1, cluster: 34}]", "id": "QUESTION 3(B)"}, {"entity_type": "TOPIC", "description": "Examining strategies in Reinforcement Learning that balance the trade-offs between exploration and exploitation.", "source_id": "chunk-31aa9c841d724e7a958338c15575adcb", "clusters": "[{level: 0, cluster: 1}, {level: 1, cluster: 12}]", "id": "EXPLORATION-EXPLOITATION IN REINFORCEMENT LEARNING"}, {"entity_type": "SUBTOPIC", "description": "Understanding the k-armed bandit problem, its applications, and Monte Carlo methods, including first-visit Monte Carlo Prediction, in decision-making and learning.", "source_id": "chunk-31aa9c841d724e7a958338c15575adcb", "clusters": "[{level: 0, cluster: 11}, {level: 1, cluster: 33}]", "id": "MULTI-ARMED BANDITS AND MONTE CARLO METHODS"}, {"entity_type": "QUESTION", "description": "Discuss the k-armed bandit problem, focusing on exploration-exploitation trade-offs. Discuss four practical applications of the k-armed bandit problem, across different domains, showcasing its adaptability in optimizing decision-making processes. [10 Marks]", "source_id": "chunk-31aa9c841d724e7a958338c15575adcb", "clusters": "[{level: 0, cluster: 11}, {level: 1, cluster: 33}]", "id": "QUESTION 4(A)"}, {"entity_type": "QUESTION", "description": "Describe the concept of Monte Carlo Prediction in Reinforcement Learning. Write the pseudocode for first-visit Monte Carlo Prediction. Discuss the advantage of employing Monte Carlo methods over Dynamic Programming (DP) methods specifically in the context of the blackjack game. [10 Marks]", "source_id": "chunk-31aa9c841d724e7a958338c15575adcb", "clusters": "[{level: 0, cluster: 11}, {level: 1, cluster: 33}]", "id": "QUESTION 4(B)"}, {"entity_type": "TOPIC", "description": "Delving into real-world applications and fundamental theories of Reinforcement Learning for solving complex problems.", "source_id": "chunk-31aa9c841d724e7a958338c15575adcb", "clusters": "[{level: 0, cluster: 3}]", "id": "APPLICATIONS AND THEORETICAL FOUNDATIONS IN REINFORCEMENT LEARNING"}, {"entity_type": "SUBTOPIC", "description": "Designing algorithms for Dynamic Channel Allocation, evaluating concepts of goals, rewards, returns, episodes, and discounting within RL theories.", "source_id": "chunk-31aa9c841d724e7a958338c15575adcb", "clusters": "[{level: 0, cluster: 11}, {level: 1, cluster: 32}]", "id": "DYNAMIC ALLOCATION AND FOUNDATIONS OF RL CONCEPTS"}, {"entity_type": "QUESTION", "description": "Design a Reinforcement Learning algorithm to optimize Dynamic Channel Allocation in a wireless communication network. Provide the state representation, action space, reward function, and exploration strategy. Discuss any one potential challenge in implementing such an algorithm in a real-world scenario. [10 Marks]", "source_id": "chunk-31aa9c841d724e7a958338c15575adcb", "clusters": "[{level: 0, cluster: 11}, {level: 1, cluster: 32}]", "id": "QUESTION 5(A)"}, {"entity_type": "QUESTION", "description": "In the context of reinforcement learning evaluate the concepts of Goals, Rewards, Returns, Episodes and Discounting. Discuss the conventional representations and mathematical formulations associated with Goals, Rewards, Returns, Episodes and Discounting. [10 Marks]", "source_id": "chunk-31aa9c841d724e7a958338c15575adcb", "clusters": "[{level: 0, cluster: 11}, {level: 1, cluster: 32}]", "id": "QUESTION 5(B)"}, {"entity_type": "COGNITIVE_LEVEL", "description": "This cognitive level demands the ability to design new models or processes and evaluate their efficacy.", "source_id": "chunk-31aa9c841d724e7a958338c15575adcb", "id": "DESIGN AND EVALUATION"}, {"entity_type": "TOPIC", "description": "Model-Based Reinforcement Learning involves creating a model of the environment to predict future states and rewards, enabling planning and decision-making based on simulated experiences.", "source_id": "chunk-31aa9c841d724e7a958338c15575adcb", "clusters": "[{level: 0, cluster: 8}]", "id": "MODEL-BASED RL"}, {"entity_type": "TOPIC", "description": "Model-Free Reinforcement Learning does not rely on a model of the environment; instead, it learns directly from interactions with the environment through trial and error, optimizing actions based on received rewards.", "source_id": "chunk-31aa9c841d724e7a958338c15575adcb", "clusters": "[{level: 0, cluster: 8}]", "id": "MODEL-FREE RL"}, {"entity_type": "SUBTOPIC", "description": "Iterative Policy Evaluation involves evaluating the value of a policy through repeated updates until it converges, providing an estimation of the expected return of each state under the given policy.", "source_id": "chunk-31aa9c841d724e7a958338c15575adcb", "clusters": "[{level: 0, cluster: 8}]", "id": "ITERATIVE POLICY EVALUATION"}, {"entity_type": "TOPIC", "description": "Markov Decision Processes are used in Reinforcement Learning to provide a mathematical framework for modeling decision-making in environments with stochastic transitions and rewards.", "source_id": "chunk-31aa9c841d724e7a958338c15575adcb", "clusters": "[{level: 0, cluster: 7}, {level: 1, cluster: 26}]", "id": "MARKOV DECISION PROCESSES (MDPS)"}, {"entity_type": "SUBTOPIC", "description": "UCB Action Selection is a strategy used in certain Reinforcement Learning problems, like multi-armed bandits, to balance exploration and exploitation by selecting actions based on the upper confidence bounds of the estimated values.", "source_id": "chunk-31aa9c841d724e7a958338c15575adcb", "clusters": "[{level: 0, cluster: 7}, {level: 1, cluster: 26}]", "id": "UPPER-CONFIDENCE-BOUND (UCB) ACTION SELECTION"}, {"entity_type": "SUBTOPIC", "description": "This subtopic examines the foundational concepts in Reinforcement Learning, including goal formulation, reward signals, return calculations, episode definitions, and discount factors for future rewards.", "source_id": "chunk-31aa9c841d724e7a958338c15575adcb", "clusters": "[{level: 0, cluster: 3}]", "id": "GOALS, REWARDS, RETURNS, EPISODES AND DISCOUNTING"}], "links": [{"weight": 8.0, "description": "is a prerequisite topic under", "source_id": "chunk-5e54a0db75e608a382c1ce071f140ac3", "order": 1, "source": "REINFORCEMENT LEARNING", "target": "PREREQUISITE"}, {"weight": 9.0, "description": "is a topic under", "source_id": "chunk-5e54a0db75e608a382c1ce071f140ac3", "order": 1, "source": "REINFORCEMENT LEARNING", "target": "INTRODUCTION TO REINFORCEMENT LEARNING"}, {"weight": 9.0, "description": "is a topic under", "source_id": "chunk-5e54a0db75e608a382c1ce071f140ac3", "order": 1, "source": "REINFORCEMENT LEARNING", "target": "BANDIT PROBLEMS AND ONLINE LEARNING"}, {"weight": 9.0, "description": "is a topic under", "source_id": "chunk-5e54a0db75e608a382c1ce071f140ac3", "order": 1, "source": "REINFORCEMENT LEARNING", "target": "MARKOV DECISION PROCESSES"}, {"weight": 9.0, "description": "is a topic under", "source_id": "chunk-5e54a0db75e608a382c1ce071f140ac3", "order": 1, "source": "REINFORCEMENT LEARNING", "target": "DYNAMIC PROGRAMMING"}, {"weight": 9.0, "description": "is a topic under", "source_id": "chunk-5e54a0db75e608a382c1ce071f140ac3", "order": 1, "source": "REINFORCEMENT LEARNING", "target": "MONTE CARLO METHODS AND TEMPORAL-DIFFERENCE LEARNING"}, {"weight": 9.0, "description": "is a topic under", "source_id": "chunk-5e54a0db75e608a382c1ce071f140ac3", "order": 1, "source": "REINFORCEMENT LEARNING", "target": "APPLICATIONS AND CASE STUDIES"}, {"weight": 7.0, "description": "complements", "source_id": "chunk-5e54a0db75e608a382c1ce071f140ac3", "order": 1, "source": "REINFORCEMENT LEARNING", "target": "REFERENCE MATERIALS"}, {"weight": 10.0, "description": "is a topic of", "source_id": "chunk-5d32567366bb059b9b2b2aefd5dbd87f", "order": 1, "source": "REINFORCEMENT LEARNING", "target": "FUNDAMENTALS OF REINFORCEMENT LEARNING"}, {"weight": 10.0, "description": "is a topic of", "source_id": "chunk-5d32567366bb059b9b2b2aefd5dbd87f", "order": 1, "source": "REINFORCEMENT LEARNING", "target": "REINFORCEMENT LEARNING STRATEGIES"}, {"weight": 10.0, "description": "is a topic of", "source_id": "chunk-5d32567366bb059b9b2b2aefd5dbd87f", "order": 1, "source": "REINFORCEMENT LEARNING", "target": "EXPLORATION STRATEGIES"}, {"weight": 10.0, "description": "is a topic of", "source_id": "chunk-5d32567366bb059b9b2b2aefd5dbd87f", "order": 1, "source": "REINFORCEMENT LEARNING", "target": "APPLICATIONS OF REINFORCEMENT LEARNING"}, {"weight": 10.0, "description": "is a topic of", "source_id": "chunk-5d32567366bb059b9b2b2aefd5dbd87f", "order": 1, "source": "REINFORCEMENT LEARNING", "target": "ADVANCED REINFORCEMENT LEARNING TECHNIQUES"}, {"weight": 10.0, "description": "is a topic of", "source_id": "chunk-5d32567366bb059b9b2b2aefd5dbd87f", "order": 1, "source": "REINFORCEMENT LEARNING", "target": "REINFORCEMENT LEARNING ALGORITHMS"}, {"weight": 10.0, "description": "is a topic of", "source_id": "chunk-5d32567366bb059b9b2b2aefd5dbd87f", "order": 1, "source": "REINFORCEMENT LEARNING", "target": "REINFORCEMENT LEARNING THEORY"}, {"weight": 10.0, "description": "is a topic of", "source_id": "chunk-31aa9c841d724e7a958338c15575adcb", "order": 1, "source": "REINFORCEMENT LEARNING", "target": "POLICY AND VALUE FUNCTIONS IN REINFORCEMENT LEARNING"}, {"weight": 10.0, "description": "is a topic of", "source_id": "chunk-31aa9c841d724e7a958338c15575adcb", "order": 1, "source": "REINFORCEMENT LEARNING", "target": "TYPES OF REINFORCEMENT LEARNING"}, {"weight": 10.0, "description": "is a topic of", "source_id": "chunk-31aa9c841d724e7a958338c15575adcb", "order": 1, "source": "REINFORCEMENT LEARNING", "target": "FUNDAMENTAL CONCEPTS IN REINFORCEMENT LEARNING"}, {"weight": 10.0, "description": "is a topic of", "source_id": "chunk-31aa9c841d724e7a958338c15575adcb", "order": 1, "source": "REINFORCEMENT LEARNING", "target": "EXPLORATION-EXPLOITATION IN REINFORCEMENT LEARNING"}, {"weight": 10.0, "description": "is a topic of", "source_id": "chunk-31aa9c841d724e7a958338c15575adcb", "order": 1, "source": "REINFORCEMENT LEARNING", "target": "APPLICATIONS AND THEORETICAL FOUNDATIONS IN REINFORCEMENT LEARNING"}, {"weight": 8.0, "description": "is a topic related to", "source_id": "chunk-31aa9c841d724e7a958338c15575adcb", "order": 1, "source": "REINFORCEMENT LEARNING", "target": "SARSA"}, {"weight": 8.0, "description": "is a topic related to", "source_id": "chunk-31aa9c841d724e7a958338c15575adcb", "order": 1, "source": "REINFORCEMENT LEARNING", "target": "Q-LEARNING"}, {"weight": 10.0, "description": "is a subtopic of", "source_id": "chunk-5e54a0db75e608a382c1ce071f140ac3", "order": 1, "source": "INTRODUCTION TO REINFORCEMENT LEARNING", "target": "REINFORCEMENT LEARNING ALGORITHMS"}, {"weight": 9.0, "description": "is a subtopic of", "source_id": "chunk-5d32567366bb059b9b2b2aefd5dbd87f", "order": 1, "source": "REINFORCEMENT LEARNING ALGORITHMS", "target": "Q-LEARNING"}, {"weight": 9.0, "description": "is a subtopic of", "source_id": "chunk-5d32567366bb059b9b2b2aefd5dbd87f", "order": 1, "source": "REINFORCEMENT LEARNING ALGORITHMS", "target": "SARSA"}, {"weight": 8.0, "description": "is a subtopic of", "source_id": "chunk-312b45904d3587796a9473ebd66e30d1", "order": 1, "source": "REINFORCEMENT LEARNING ALGORITHMS", "target": "SARSA VS Q-LEARNING"}, {"weight": 8.0, "description": "is a subtopic of", "source_id": "chunk-312b45904d3587796a9473ebd66e30d1", "order": 1, "source": "REINFORCEMENT LEARNING ALGORITHMS", "target": "MONTE CARLO METHODS"}, {"weight": 10.0, "description": "is a subtopic of", "source_id": "chunk-5e54a0db75e608a382c1ce071f140ac3", "order": 1, "source": "BANDIT PROBLEMS AND ONLINE LEARNING", "target": "AN N-ARMED BANDIT PROBLEM"}, {"weight": 10.0, "description": "is a subtopic of", "source_id": "chunk-5e54a0db75e608a382c1ce071f140ac3", "order": 1, "source": "BANDIT PROBLEMS AND ONLINE LEARNING", "target": "ACTION-VALUE METHODS"}, {"weight": 10.0, "description": "is a subtopic of", "source_id": "chunk-5e54a0db75e608a382c1ce071f140ac3", "order": 1, "source": "BANDIT PROBLEMS AND ONLINE LEARNING", "target": "OPTIMISTIC INITIAL VALUES"}, {"weight": 10.0, "description": "is a subtopic of", "source_id": "chunk-5e54a0db75e608a382c1ce071f140ac3", "order": 1, "source": "BANDIT PROBLEMS AND ONLINE LEARNING", "target": "UPPER-CONFIDENCE-BOUND ACTION SELECTION"}, {"weight": 10.0, "description": "is a subtopic of", "source_id": "chunk-5e54a0db75e608a382c1ce071f140ac3", "order": 1, "source": "BANDIT PROBLEMS AND ONLINE LEARNING", "target": "GRADIENT BANDITS"}, {"weight": 8.0, "description": "is a concept within", "source_id": "chunk-5d32567366bb059b9b2b2aefd5dbd87f", "order": 1, "source": "OPTIMISTIC INITIAL VALUES", "target": "INITIAL VALUE SETTINGS"}, {"weight": 10.0, "description": "is a subtopic of", "source_id": "chunk-5e54a0db75e608a382c1ce071f140ac3", "order": 1, "source": "MARKOV DECISION PROCESSES", "target": "THE AGENT-ENVIRONMENT INTERFACE"}, {"weight": 10.0, "description": "is a subtopic of", "source_id": "chunk-5e54a0db75e608a382c1ce071f140ac3", "order": 1, "source": "MARKOV DECISION PROCESSES", "target": "GOALS AND REWARDS"}, {"weight": 10.0, "description": "is a subtopic of", "source_id": "chunk-5e54a0db75e608a382c1ce071f140ac3", "order": 1, "source": "MARKOV DECISION PROCESSES", "target": "RETURNS AND MARKOV PROPERTIES"}, {"weight": 10.0, "description": "is a subtopic of", "source_id": "chunk-5e54a0db75e608a382c1ce071f140ac3", "order": 1, "source": "MARKOV DECISION PROCESSES", "target": "MARKOV DECISION PROCESS"}, {"weight": 10.0, "description": "is a subtopic of", "source_id": "chunk-5e54a0db75e608a382c1ce071f140ac3", "order": 1, "source": "MARKOV DECISION PROCESSES", "target": "VALUE FUNCTIONS AND OPTIMAL VALUE FUNCTIONS"}, {"weight": 9.0, "description": "is a subtopic of", "source_id": "chunk-312b45904d3587796a9473ebd66e30d1", "order": 1, "source": "MARKOV DECISION PROCESSES", "target": "FUNDAMENTALS OF REINFORCEMENT LEARNING"}, {"weight": 9.0, "description": "is a question about", "source_id": "chunk-312b45904d3587796a9473ebd66e30d1", "order": 1, "source": "MARKOV DECISION PROCESSES", "target": "EXPLAIN THE MARKOV PROPERTIES AND THEIR ROLE IN CONSTRUCTING MARKOV DECISION PROCESSES (MDPS) IN REINFORCEMENT LEARNING. FORMULATE AN MDP SCENARIO DEPICTING A BOT COLLECTING EMPTY SODA CANS IN AN OFFICE ENVIRONMENT AS AN ILLUSTRATION OF HOW MARKOV PROPERTIES ARE APPLIED TO MODEL COMPLEX DECISION-MAKING TASKS. (10 MARKS)"}, {"weight": 9.0, "description": "has difficulty level", "source_id": "chunk-312b45904d3587796a9473ebd66e30d1", "order": 1, "source": "MARKOV DECISION PROCESSES", "target": "HARD"}, {"weight": 9.0, "description": "requires cognitive level", "source_id": "chunk-312b45904d3587796a9473ebd66e30d1", "order": 1, "source": "MARKOV DECISION PROCESSES", "target": "APPLICATION"}, {"weight": 10.0, "description": "is a subtopic of", "source_id": "chunk-5e54a0db75e608a382c1ce071f140ac3", "order": 1, "source": "DYNAMIC PROGRAMMING", "target": "POLICY EVALUATION (PREDICTION)"}, {"weight": 10.0, "description": "is a subtopic of", "source_id": "chunk-5e54a0db75e608a382c1ce071f140ac3", "order": 1, "source": "DYNAMIC PROGRAMMING", "target": "POLICY IMPROVEMENT"}, {"weight": 10.0, "description": "is a subtopic of", "source_id": "chunk-5e54a0db75e608a382c1ce071f140ac3", "order": 1, "source": "DYNAMIC PROGRAMMING", "target": "POLICY ITERATION"}, {"weight": 10.0, "description": "is a subtopic of", "source_id": "chunk-5e54a0db75e608a382c1ce071f140ac3", "order": 1, "source": "DYNAMIC PROGRAMMING", "target": "VALUE ITERATION"}, {"weight": 10.0, "description": "is a subtopic of", "source_id": "chunk-5e54a0db75e608a382c1ce071f140ac3", "order": 1, "source": "DYNAMIC PROGRAMMING", "target": "ASYNCHRONOUS DYNAMIC PROGRAMMING"}, {"weight": 10.0, "description": "is a subtopic of", "source_id": "chunk-5e54a0db75e608a382c1ce071f140ac3", "order": 1, "source": "DYNAMIC PROGRAMMING", "target": "GENERALIZED POLICY ITERATION"}, {"weight": 18.0, "description": "is a subtopic of<SEP>is a subtopic of", "source_id": "chunk-312b45904d3587796a9473ebd66e30d1<SEP>chunk-5d32567366bb059b9b2b2aefd5dbd87f", "order": 1, "source": "POLICY ITERATION", "target": "REINFORCEMENT LEARNING THEORY"}, {"weight": 8.0, "description": "is a question about", "source_id": "chunk-5d32567366bb059b9b2b2aefd5dbd87f", "order": 1, "source": "POLICY ITERATION", "target": "QUESTION LA"}, {"weight": 9.0, "description": "underpins the process of", "source_id": "chunk-5d32567366bb059b9b2b2aefd5dbd87f", "order": 1, "source": "POLICY ITERATION", "target": "POLICY IMPROVEMENT THEOREM"}, {"weight": 8.0, "description": "has difficulty level", "source_id": "chunk-312b45904d3587796a9473ebd66e30d1", "order": 1, "source": "POLICY ITERATION", "target": "HARD"}, {"weight": 8.0, "description": "requires cognitive level", "source_id": "chunk-312b45904d3587796a9473ebd66e30d1", "order": 1, "source": "POLICY ITERATION", "target": "COMPREHENSION"}, {"weight": 10.0, "description": "is a subtopic of", "source_id": "chunk-5e54a0db75e608a382c1ce071f140ac3", "order": 1, "source": "MONTE CARLO METHODS AND TEMPORAL-DIFFERENCE LEARNING", "target": "MONTE CARLO PREDICTION"}, {"weight": 10.0, "description": "is a subtopic of", "source_id": "chunk-5e54a0db75e608a382c1ce071f140ac3", "order": 1, "source": "MONTE CARLO METHODS AND TEMPORAL-DIFFERENCE LEARNING", "target": "MONTE CARLO ESTIMATION OF ACTION VALUES"}, {"weight": 10.0, "description": "is a subtopic of", "source_id": "chunk-5e54a0db75e608a382c1ce071f140ac3", "order": 1, "source": "MONTE CARLO METHODS AND TEMPORAL-DIFFERENCE LEARNING", "target": "MONTE CARLO CONTROL"}, {"weight": 10.0, "description": "is a subtopic of", "source_id": "chunk-5e54a0db75e608a382c1ce071f140ac3", "order": 1, "source": "MONTE CARLO METHODS AND TEMPORAL-DIFFERENCE LEARNING", "target": "TD PREDICTION"}, {"weight": 10.0, "description": "is a subtopic of", "source_id": "chunk-5e54a0db75e608a382c1ce071f140ac3", "order": 1, "source": "MONTE CARLO METHODS AND TEMPORAL-DIFFERENCE LEARNING", "target": "TD CONTROL USING Q-LEARNING"}, {"weight": 9.0, "description": "is integral to", "source_id": "chunk-31aa9c841d724e7a958338c15575adcb", "order": 1, "source": "MONTE CARLO PREDICTION", "target": "EXPLORATION-EXPLOITATION IN REINFORCEMENT LEARNING"}, {"weight": 10.0, "description": "is a subtopic of", "source_id": "chunk-5e54a0db75e608a382c1ce071f140ac3", "order": 1, "source": "APPLICATIONS AND CASE STUDIES", "target": "ELEVATOR DISPATCHING"}, {"weight": 10.0, "description": "is a subtopic of", "source_id": "chunk-5e54a0db75e608a382c1ce071f140ac3", "order": 1, "source": "APPLICATIONS AND CASE STUDIES", "target": "DYNAMIC CHANNEL ALLOCATION"}, {"weight": 10.0, "description": "is a subtopic of", "source_id": "chunk-5e54a0db75e608a382c1ce071f140ac3", "order": 1, "source": "APPLICATIONS AND CASE STUDIES", "target": "JOB-SHOP SCHEDULING"}, {"weight": 9.0, "description": "includes", "source_id": "chunk-31aa9c841d724e7a958338c15575adcb", "order": 1, "source": "DYNAMIC CHANNEL ALLOCATION", "target": "APPLICATIONS AND THEORETICAL FOUNDATIONS IN REINFORCEMENT LEARNING"}, {"weight": 5.0, "description": "is referenced by", "source_id": "chunk-5e54a0db75e608a382c1ce071f140ac3", "order": 1, "source": "REFERENCE MATERIALS", "target": "REINFORCEMENT LEARNING: AN INTRODUCTION"}, {"weight": 5.0, "description": "is referenced by", "source_id": "chunk-5e54a0db75e608a382c1ce071f140ac3", "order": 1, "source": "REFERENCE MATERIALS", "target": "THE REINFORCEMENT LEARNING WORKSHOP"}, {"weight": 5.0, "description": "is referenced by", "source_id": "chunk-5e54a0db75e608a382c1ce071f140ac3", "order": 1, "source": "REFERENCE MATERIALS", "target": "REINFORCEMENT LEARNING INDUSTRIAL APPLICATIONS"}, {"weight": 5.0, "description": "is referenced by", "source_id": "chunk-5e54a0db75e608a382c1ce071f140ac3", "order": 1, "source": "REFERENCE MATERIALS", "target": "PRACTICAL REINFORCEMENT LEARNING"}, {"weight": 8.0, "description": "is a question about", "source_id": "chunk-5d32567366bb059b9b2b2aefd5dbd87f", "order": 1, "source": "Q-LEARNING", "target": "QUESTION 3A"}, {"weight": 8.0, "description": "characterizes the learning type", "source_id": "chunk-5d32567366bb059b9b2b2aefd5dbd87f", "order": 1, "source": "Q-LEARNING", "target": "OFF-POLICY LEARNING"}, {"weight": 9.0, "description": "is a subtopic of", "source_id": "chunk-5d32567366bb059b9b2b2aefd5dbd87f", "order": 1, "source": "FUNDAMENTALS OF REINFORCEMENT LEARNING", "target": "COMPARISON WITH OTHER LEARNING PARADIGMS"}, {"weight": 9.0, "description": "is a subtopic of", "source_id": "chunk-5d32567366bb059b9b2b2aefd5dbd87f", "order": 1, "source": "FUNDAMENTALS OF REINFORCEMENT LEARNING", "target": "MDP"}, {"weight": 9.0, "description": "is a subtopic of", "source_id": "chunk-5d32567366bb059b9b2b2aefd5dbd87f", "order": 1, "source": "FUNDAMENTALS OF REINFORCEMENT LEARNING", "target": "DISCOUNT FACTOR"}, {"weight": 8.0, "description": "is a question about", "source_id": "chunk-5d32567366bb059b9b2b2aefd5dbd87f", "order": 1, "source": "COMPARISON WITH OTHER LEARNING PARADIGMS", "target": "QUESTION 1A"}, {"weight": 8.0, "description": "is a question about", "source_id": "chunk-5d32567366bb059b9b2b2aefd5dbd87f", "order": 1, "source": "MDP", "target": "QUESTION 1F"}, {"weight": 7.0, "description": "is a concept related to", "source_id": "chunk-5d32567366bb059b9b2b2aefd5dbd87f", "order": 1, "source": "MDP", "target": "GOALS"}, {"weight": 7.0, "description": "is a concept related to", "source_id": "chunk-5d32567366bb059b9b2b2aefd5dbd87f", "order": 1, "source": "MDP", "target": "REWARDS"}, {"weight": 7.0, "description": "is a concept related to", "source_id": "chunk-5d32567366bb059b9b2b2aefd5dbd87f", "order": 1, "source": "MDP", "target": "RETURNS"}, {"weight": 7.0, "description": "is a concept related to", "source_id": "chunk-5d32567366bb059b9b2b2aefd5dbd87f", "order": 1, "source": "MDP", "target": "EPISODES"}, {"weight": 9.0, "description": "is a subtopic of", "source_id": "chunk-5d32567366bb059b9b2b2aefd5dbd87f", "order": 1, "source": "REINFORCEMENT LEARNING STRATEGIES", "target": "POLICY TYPES"}, {"weight": 8.0, "description": "is a question about", "source_id": "chunk-5d32567366bb059b9b2b2aefd5dbd87f", "order": 1, "source": "POLICY TYPES", "target": "QUESTION 1B"}, {"weight": 9.0, "description": "is a subtopic of", "source_id": "chunk-5d32567366bb059b9b2b2aefd5dbd87f", "order": 1, "source": "EXPLORATION STRATEGIES", "target": "INITIAL VALUE SETTINGS"}, {"weight": 9.0, "description": "is a subtopic of", "source_id": "chunk-5d32567366bb059b9b2b2aefd5dbd87f", "order": 1, "source": "EXPLORATION STRATEGIES", "target": "MULTI-ARMED BANDITS"}, {"weight": 8.0, "description": "is a subtopic of", "source_id": "chunk-312b45904d3587796a9473ebd66e30d1", "order": 1, "source": "EXPLORATION STRATEGIES", "target": "UCB ACTION SELECTION"}, {"weight": 8.0, "description": "is a subtopic of", "source_id": "chunk-312b45904d3587796a9473ebd66e30d1", "order": 1, "source": "EXPLORATION STRATEGIES", "target": "K-ARMED BANDIT PROBLEM"}, {"weight": 8.0, "description": "is a question about", "source_id": "chunk-5d32567366bb059b9b2b2aefd5dbd87f", "order": 1, "source": "INITIAL VALUE SETTINGS", "target": "QUESTION 1D"}, {"weight": 8.0, "description": "is a question about", "source_id": "chunk-5d32567366bb059b9b2b2aefd5dbd87f", "order": 1, "source": "MULTI-ARMED BANDITS", "target": "QUESTION 2A"}, {"weight": 8.0, "description": "is a method within", "source_id": "chunk-5d32567366bb059b9b2b2aefd5dbd87f", "order": 1, "source": "MULTI-ARMED BANDITS", "target": "UPPER-CONFIDENCE-BOUND (UCB)"}, {"weight": 9.0, "description": "is a subtopic of", "source_id": "chunk-5d32567366bb059b9b2b2aefd5dbd87f", "order": 1, "source": "APPLICATIONS OF REINFORCEMENT LEARNING", "target": "ROBOTICS"}, {"weight": 8.0, "description": "is a question about", "source_id": "chunk-5d32567366bb059b9b2b2aefd5dbd87f", "order": 1, "source": "ROBOTICS", "target": "QUESTION 1E"}, {"weight": 9.0, "description": "is a subtopic of", "source_id": "chunk-5d32567366bb059b9b2b2aefd5dbd87f", "order": 1, "source": "ADVANCED REINFORCEMENT LEARNING TECHNIQUES", "target": "BANDIT ALGORITHMS"}, {"weight": 8.0, "description": "is a question about", "source_id": "chunk-5d32567366bb059b9b2b2aefd5dbd87f", "order": 1, "source": "BANDIT ALGORITHMS", "target": "QUESTION 2B"}, {"weight": 8.0, "description": "is a type of", "source_id": "chunk-5d32567366bb059b9b2b2aefd5dbd87f", "order": 1, "source": "BANDIT ALGORITHMS", "target": "GRADIENT BANDIT ALGORITHMS"}, {"weight": 8.0, "description": "is a question about", "source_id": "chunk-5d32567366bb059b9b2b2aefd5dbd87f", "order": 1, "source": "SARSA", "target": "QUESTION 3B"}, {"weight": 8.0, "description": "characterizes the learning type", "source_id": "chunk-5d32567366bb059b9b2b2aefd5dbd87f", "order": 1, "source": "SARSA", "target": "ON-POLICY LEARNING"}, {"weight": 8.0, "description": "is a question about", "source_id": "chunk-5d32567366bb059b9b2b2aefd5dbd87f", "order": 1, "source": "QUESTION 1C", "target": "DISCOUNT FACTOR"}, {"weight": 8.0, "description": "is a question about", "source_id": "chunk-5d32567366bb059b9b2b2aefd5dbd87f", "order": 1, "source": "QUESTION LB", "target": "SARSA VS Q-LEARNING"}, {"weight": 8.0, "description": "is a technique used in", "source_id": "chunk-5d32567366bb059b9b2b2aefd5dbd87f", "order": 1, "source": "GRADIENT BANDIT ALGORITHMS", "target": "SOFTMAX ACTION SELECTION"}, {"weight": 8.0, "description": "influences performance of", "source_id": "chunk-5d32567366bb059b9b2b2aefd5dbd87f", "order": 1, "source": "GRADIENT BANDIT ALGORITHMS", "target": "LEARNING RATE"}, {"weight": 10.0, "description": "is a question about", "source_id": "chunk-312b45904d3587796a9473ebd66e30d1", "order": 1, "source": "SARSA VS Q-LEARNING", "target": "COMPARE SARSA AND Q-LEARNING, HIGHLIGHTING THE DIFFERENCE BETWEEN ON-POLICY AND OFF-POLICY METHODS. PROVIDE A SUITABLE EXAMPLE. (10 MARKS)"}, {"weight": 9.0, "description": "has difficulty level", "source_id": "chunk-312b45904d3587796a9473ebd66e30d1", "order": 1, "source": "SARSA VS Q-LEARNING", "target": "MEDIUM"}, {"weight": 9.0, "description": "requires cognitive level", "source_id": "chunk-312b45904d3587796a9473ebd66e30d1", "order": 1, "source": "SARSA VS Q-LEARNING", "target": "ANALYSIS"}, {"weight": 8.0, "description": "is a subtopic of", "source_id": "chunk-312b45904d3587796a9473ebd66e30d1", "order": 1, "source": "MODEL-BASED VS MODEL-FREE RL", "target": "REINFORCEMENT LEARNING PARADIGMS"}, {"weight": 10.0, "description": "is a question about", "source_id": "chunk-312b45904d3587796a9473ebd66e30d1", "order": 1, "source": "MODEL-BASED VS MODEL-FREE RL", "target": "DIFFERENTIATE BETWEEN MODEL-BASED AND MODEL-FREE TYPES OF REINFORCEMENT LEARNING (RL). DISCUSS THE ADVANTAGES AND LIMITATIONS OF EACH APPROACH, PROVIDING REAL-WORLD EXAMPLES WHERE EACH TYPE WOULD BE MOST SUITABLE. (10 MARKS)"}, {"weight": 9.0, "description": "has difficulty level", "source_id": "chunk-312b45904d3587796a9473ebd66e30d1", "order": 1, "source": "MODEL-BASED VS MODEL-FREE RL", "target": "MEDIUM"}, {"weight": 9.0, "description": "requires cognitive level", "source_id": "chunk-312b45904d3587796a9473ebd66e30d1", "order": 1, "source": "MODEL-BASED VS MODEL-FREE RL", "target": "ANALYSIS"}, {"weight": 7.0, "description": "is a subtopic of", "source_id": "chunk-312b45904d3587796a9473ebd66e30d1", "order": 1, "source": "POLICY EVALUATION", "target": "REINFORCEMENT LEARNING TECHNIQUES"}, {"weight": 9.0, "description": "is a question about", "source_id": "chunk-312b45904d3587796a9473ebd66e30d1", "order": 1, "source": "POLICY EVALUATION", "target": "DISCUSS THE ITERATIVE POLICY EVALUATION WITH THE HELP OF A SUITABLE EXAMPLE. (10 MARKS)"}, {"weight": 9.0, "description": "has difficulty level", "source_id": "chunk-312b45904d3587796a9473ebd66e30d1", "order": 1, "source": "POLICY EVALUATION", "target": "MEDIUM"}, {"weight": 9.0, "description": "requires cognitive level", "source_id": "chunk-312b45904d3587796a9473ebd66e30d1", "order": 1, "source": "POLICY EVALUATION", "target": "COMPREHENSION"}, {"weight": 9.0, "description": "is a question about", "source_id": "chunk-312b45904d3587796a9473ebd66e30d1", "order": 1, "source": "UCB ACTION SELECTION", "target": "EXPLORE UPPER-CONFIDENCE-BOUND (UCB) ACTION SELECTION IN MULTI-ARMED BANDITS. ANALYZE UCB\'S FORMULA AND ADDRESS POTENTIAL APPLICATION CHALLENGES. (10 MARKS)"}, {"weight": 9.0, "description": "has difficulty level", "source_id": "chunk-312b45904d3587796a9473ebd66e30d1", "order": 1, "source": "UCB ACTION SELECTION", "target": "HARD"}, {"weight": 9.0, "description": "requires cognitive level", "source_id": "chunk-312b45904d3587796a9473ebd66e30d1", "order": 1, "source": "UCB ACTION SELECTION", "target": "ANALYSIS"}, {"weight": 9.0, "description": "is a question about", "source_id": "chunk-312b45904d3587796a9473ebd66e30d1", "order": 1, "source": "K-ARMED BANDIT PROBLEM", "target": "DISCUSS THE K-ARMED BANDIT PROBLEM, FOCUSING ON EXPLORATION-EXPLOITATION TRADE-OFFS. DISCUSS FOUR PRACTICAL APPLICATIONS OF THE K-ARMED BANDIT PROBLEM, ACROSS DIFFERENT DOMAINS, SHOWCASING ITS ADAPTABILITY IN OPTIMIZING DECISION-MAKING PROCESSES. (10 MARKS)"}, {"weight": 9.0, "description": "has difficulty level", "source_id": "chunk-312b45904d3587796a9473ebd66e30d1", "order": 1, "source": "K-ARMED BANDIT PROBLEM", "target": "MEDIUM"}, {"weight": 9.0, "description": "requires cognitive level", "source_id": "chunk-312b45904d3587796a9473ebd66e30d1", "order": 1, "source": "K-ARMED BANDIT PROBLEM", "target": "ANALYSIS"}, {"weight": 9.0, "description": "is a case study for", "source_id": "chunk-31aa9c841d724e7a958338c15575adcb", "order": 1, "source": "K-ARMED BANDIT PROBLEM", "target": "EXPLORATION-EXPLOITATION IN REINFORCEMENT LEARNING"}, {"weight": 10.0, "description": "is a question about", "source_id": "chunk-312b45904d3587796a9473ebd66e30d1", "order": 1, "source": "MONTE CARLO METHODS", "target": "DESCRIBE THE CONCEPT OF MONTE CARLO PREDICTION IN REINFORCEMENT LEARNING. WRITE THE PSEUDOCODE FOR FIRST-VISIT MONTE CARLO PREDICTION. DISCUSS THE ADVANTAGE OF EMPLOYING MONTE CARLO METHODS OVER DYNAMIC PROGRAMMING (DP) METHODS SPECIFICALLY IN THE CONTEXT OF THE BLACKJACK GAME. (10 MARKS)"}, {"weight": 9.0, "description": "has difficulty level", "source_id": "chunk-312b45904d3587796a9473ebd66e30d1", "order": 1, "source": "MONTE CARLO METHODS", "target": "MEDIUM"}, {"weight": 9.0, "description": "requires cognitive level", "source_id": "chunk-312b45904d3587796a9473ebd66e30d1", "order": 1, "source": "MONTE CARLO METHODS", "target": "APPLICATION"}, {"weight": 9.0, "description": "is a subtopic of", "source_id": "chunk-31aa9c841d724e7a958338c15575adcb", "order": 1, "source": "POLICY AND VALUE FUNCTIONS IN REINFORCEMENT LEARNING", "target": "POLICY IMPROVEMENT THEOREM AND COMPARISON OF LEARNING METHODS"}, {"weight": 8.0, "description": "is a question about", "source_id": "chunk-31aa9c841d724e7a958338c15575adcb", "order": 1, "source": "POLICY IMPROVEMENT THEOREM AND COMPARISON OF LEARNING METHODS", "target": "QUESTION 1(A)"}, {"weight": 8.0, "description": "is a question about", "source_id": "chunk-31aa9c841d724e7a958338c15575adcb", "order": 1, "source": "POLICY IMPROVEMENT THEOREM AND COMPARISON OF LEARNING METHODS", "target": "QUESTION 1(B)"}, {"weight": 7.0, "description": "explores related concepts in the same subtopic", "source_id": "chunk-31aa9c841d724e7a958338c15575adcb", "order": 1, "source": "QUESTION 1(A)", "target": "QUESTION 1(B)"}, {"weight": 10.0, "description": "appears in paper", "source_id": "chunk-31aa9c841d724e7a958338c15575adcb", "order": 1, "source": "QUESTION 1(A)", "target": "QUESTION PAPER 2024 SEMESTER VIII"}, {"weight": 10.0, "description": "appears in paper", "source_id": "chunk-31aa9c841d724e7a958338c15575adcb", "order": 1, "source": "QUESTION 1(B)", "target": "QUESTION PAPER 2024 SEMESTER VIII"}, {"weight": 10.0, "description": "appears in paper", "source_id": "chunk-31aa9c841d724e7a958338c15575adcb", "order": 1, "source": "QUESTION PAPER 2024 SEMESTER VIII", "target": "QUESTION 2(A)"}, {"weight": 10.0, "description": "appears in paper", "source_id": "chunk-31aa9c841d724e7a958338c15575adcb", "order": 1, "source": "QUESTION PAPER 2024 SEMESTER VIII", "target": "QUESTION 2(B)"}, {"weight": 10.0, "description": "appears in paper", "source_id": "chunk-31aa9c841d724e7a958338c15575adcb", "order": 1, "source": "QUESTION PAPER 2024 SEMESTER VIII", "target": "QUESTION 3(A)"}, {"weight": 10.0, "description": "appears in paper", "source_id": "chunk-31aa9c841d724e7a958338c15575adcb", "order": 1, "source": "QUESTION PAPER 2024 SEMESTER VIII", "target": "QUESTION 3(B)"}, {"weight": 10.0, "description": "appears in paper", "source_id": "chunk-31aa9c841d724e7a958338c15575adcb", "order": 1, "source": "QUESTION PAPER 2024 SEMESTER VIII", "target": "QUESTION 4(A)"}, {"weight": 10.0, "description": "appears in paper", "source_id": "chunk-31aa9c841d724e7a958338c15575adcb", "order": 1, "source": "QUESTION PAPER 2024 SEMESTER VIII", "target": "QUESTION 4(B)"}, {"weight": 10.0, "description": "appears in paper", "source_id": "chunk-31aa9c841d724e7a958338c15575adcb", "order": 1, "source": "QUESTION PAPER 2024 SEMESTER VIII", "target": "QUESTION 5(A)"}, {"weight": 10.0, "description": "appears in paper", "source_id": "chunk-31aa9c841d724e7a958338c15575adcb", "order": 1, "source": "QUESTION PAPER 2024 SEMESTER VIII", "target": "QUESTION 5(B)"}, {"weight": 9.0, "description": "is a subtopic of", "source_id": "chunk-31aa9c841d724e7a958338c15575adcb", "order": 1, "source": "TYPES OF REINFORCEMENT LEARNING", "target": "MODEL-BASED VS MODEL-FREE RL AND POLICY EVALUATION"}, {"weight": 9.0, "description": "is a detailed examination of", "source_id": "chunk-31aa9c841d724e7a958338c15575adcb", "order": 1, "source": "TYPES OF REINFORCEMENT LEARNING", "target": "MODEL-BASED RL"}, {"weight": 9.0, "description": "is a detailed examination of", "source_id": "chunk-31aa9c841d724e7a958338c15575adcb", "order": 1, "source": "TYPES OF REINFORCEMENT LEARNING", "target": "MODEL-FREE RL"}, {"weight": 8.0, "description": "includes concepts on", "source_id": "chunk-31aa9c841d724e7a958338c15575adcb", "order": 1, "source": "TYPES OF REINFORCEMENT LEARNING", "target": "ITERATIVE POLICY EVALUATION"}, {"weight": 8.0, "description": "is a question about", "source_id": "chunk-31aa9c841d724e7a958338c15575adcb", "order": 1, "source": "MODEL-BASED VS MODEL-FREE RL AND POLICY EVALUATION", "target": "QUESTION 2(A)"}, {"weight": 8.0, "description": "is a question about", "source_id": "chunk-31aa9c841d724e7a958338c15575adcb", "order": 1, "source": "MODEL-BASED VS MODEL-FREE RL AND POLICY EVALUATION", "target": "QUESTION 2(B)"}, {"weight": 7.0, "description": "explores related concepts in the same subtopic", "source_id": "chunk-31aa9c841d724e7a958338c15575adcb", "order": 1, "source": "QUESTION 2(A)", "target": "QUESTION 2(B)"}, {"weight": 9.0, "description": "is a subtopic of", "source_id": "chunk-31aa9c841d724e7a958338c15575adcb", "order": 1, "source": "FUNDAMENTAL CONCEPTS IN REINFORCEMENT LEARNING", "target": "MARKOV DECISION PROCESSES AND ACTION SELECTION STRATEGIES"}, {"weight": 10.0, "description": "includes", "source_id": "chunk-31aa9c841d724e7a958338c15575adcb", "order": 1, "source": "FUNDAMENTAL CONCEPTS IN REINFORCEMENT LEARNING", "target": "MARKOV DECISION PROCESSES (MDPS)"}, {"weight": 10.0, "description": "includes", "source_id": "chunk-31aa9c841d724e7a958338c15575adcb", "order": 1, "source": "FUNDAMENTAL CONCEPTS IN REINFORCEMENT LEARNING", "target": "UPPER-CONFIDENCE-BOUND (UCB) ACTION SELECTION"}, {"weight": 8.0, "description": "is a question about", "source_id": "chunk-31aa9c841d724e7a958338c15575adcb", "order": 1, "source": "MARKOV DECISION PROCESSES AND ACTION SELECTION STRATEGIES", "target": "QUESTION 3(A)"}, {"weight": 8.0, "description": "is a question about", "source_id": "chunk-31aa9c841d724e7a958338c15575adcb", "order": 1, "source": "MARKOV DECISION PROCESSES AND ACTION SELECTION STRATEGIES", "target": "QUESTION 3(B)"}, {"weight": 7.0, "description": "explores related concepts in the same subtopic", "source_id": "chunk-31aa9c841d724e7a958338c15575adcb", "order": 1, "source": "QUESTION 3(A)", "target": "QUESTION 3(B)"}, {"weight": 9.0, "description": "is a subtopic of", "source_id": "chunk-31aa9c841d724e7a958338c15575adcb", "order": 1, "source": "EXPLORATION-EXPLOITATION IN REINFORCEMENT LEARNING", "target": "MULTI-ARMED BANDITS AND MONTE CARLO METHODS"}, {"weight": 8.0, "description": "is a question about", "source_id": "chunk-31aa9c841d724e7a958338c15575adcb", "order": 1, "source": "MULTI-ARMED BANDITS AND MONTE CARLO METHODS", "target": "QUESTION 4(A)"}, {"weight": 8.0, "description": "is a question about", "source_id": "chunk-31aa9c841d724e7a958338c15575adcb", "order": 1, "source": "MULTI-ARMED BANDITS AND MONTE CARLO METHODS", "target": "QUESTION 4(B)"}, {"weight": 7.0, "description": "explores related concepts in the same subtopic", "source_id": "chunk-31aa9c841d724e7a958338c15575adcb", "order": 1, "source": "QUESTION 4(A)", "target": "QUESTION 4(B)"}, {"weight": 9.0, "description": "is a subtopic of", "source_id": "chunk-31aa9c841d724e7a958338c15575adcb", "order": 1, "source": "APPLICATIONS AND THEORETICAL FOUNDATIONS IN REINFORCEMENT LEARNING", "target": "DYNAMIC ALLOCATION AND FOUNDATIONS OF RL CONCEPTS"}, {"weight": 9.0, "description": "discusses foundational aspects of", "source_id": "chunk-31aa9c841d724e7a958338c15575adcb", "order": 1, "source": "APPLICATIONS AND THEORETICAL FOUNDATIONS IN REINFORCEMENT LEARNING", "target": "GOALS, REWARDS, RETURNS, EPISODES AND DISCOUNTING"}, {"weight": 8.0, "description": "is a question about", "source_id": "chunk-31aa9c841d724e7a958338c15575adcb", "order": 1, "source": "DYNAMIC ALLOCATION AND FOUNDATIONS OF RL CONCEPTS", "target": "QUESTION 5(A)"}, {"weight": 8.0, "description": "is a question about", "source_id": "chunk-31aa9c841d724e7a958338c15575adcb", "order": 1, "source": "DYNAMIC ALLOCATION AND FOUNDATIONS OF RL CONCEPTS", "target": "QUESTION 5(B)"}, {"weight": 7.0, "description": "explores related concepts in the same subtopic", "source_id": "chunk-31aa9c841d724e7a958338c15575adcb", "order": 1, "source": "QUESTION 5(A)", "target": "QUESTION 5(B)"}]}