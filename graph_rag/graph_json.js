var graphJson = {"directed": false, "multigraph": false, "graph": {"node_default": {}, "edge_default": {}}, "nodes": [{"entity_type": "SUBJECT", "description": "This course offers a detailed exploration into Reinforcement Learning (RL), covering its key features, algorithms, and applications. Students will learn about various RL algorithms such as Q-Learning and SARSA and their applications in theoretical and practical contexts. Prior knowledge in probability distributions, expected values, and basic linear algebra is required.<SEP>This course provides an in-depth exploration of Reinforcement Learning (RL), including foundational concepts, algorithms, and practical applications. It covers essential topics such as bandit problems, Markov Decision Processes, dynamic programming, Monte Carlo methods, and temporal-difference learning. The course includes case studies to illustrate the application of RL in real-world scenarios.", "source_id": "chunk-6a1bb7cfdac4c1f3e0630b5d05d61266<SEP>chunk-d28a8b847f5857eadd394474c6ecf287", "clusters": "[{level: 0, cluster: 4}]", "id": "ADDO8013 REINFORCEMENT LEARNING"}, {"entity_type": "TOPIC", "description": "Covers probability distributions and expected values, and basic linear algebra, including inner products.", "source_id": "chunk-d28a8b847f5857eadd394474c6ecf287", "clusters": "[{level: 0, cluster: 4}]", "id": "PREREQUISITE"}, {"entity_type": "TOPIC", "description": "Introduction to key features and elements of Reinforcement Learning (RL), different types of RL, and rewards.", "source_id": "chunk-d28a8b847f5857eadd394474c6ecf287", "clusters": "[{level: 0, cluster: 4}]", "id": "INTRODUCTION TO REINFORCEMENT LEARNING"}, {"entity_type": "SUBTOPIC", "description": "Introduction to RL algorithms such as Q-Learning and State Action Reward State Action (SARSA).", "source_id": "chunk-d28a8b847f5857eadd394474c6ecf287", "clusters": "[{level: 0, cluster: 4}]", "id": "REINFORCEMENT LEARNING ALGORITHMS"}, {"entity_type": "TOPIC", "description": "Examination of an n-Armed Bandit Problem and different approaches to addressing it.", "source_id": "chunk-d28a8b847f5857eadd394474c6ecf287", "clusters": "[{level: 0, cluster: 0}]", "id": "BANDIT PROBLEMS AND ONLINE LEARNING"}, {"entity_type": "SUBTOPIC", "description": "Exploration of the structure and strategy behind the n-Armed Bandit Problem.", "source_id": "chunk-d28a8b847f5857eadd394474c6ecf287", "clusters": "[{level: 0, cluster: 0}]", "id": "AN N-ARMED BANDIT PROBLEM"}, {"entity_type": "SUBTOPIC", "description": "Analysis of methods to evaluate actions based on value.", "source_id": "chunk-d28a8b847f5857eadd394474c6ecf287", "clusters": "[{level: 0, cluster: 0}]", "id": "ACTION-VALUE METHODS"}, {"entity_type": "SUBTOPIC", "description": "Methods for addressing changes over time in problems.", "source_id": "chunk-d28a8b847f5857eadd394474c6ecf287", "clusters": "[{level: 0, cluster: 0}]", "id": "TRACKING A NONSTATIONARY PROBLEM"}, {"entity_type": "SUBTOPIC", "description": "The concept of starting with positive assumptions to enhance exploration.", "source_id": "chunk-d28a8b847f5857eadd394474c6ecf287", "clusters": "[{level: 0, cluster: 0}]", "id": "OPTIMISTIC INITIAL VALUES"}, {"entity_type": "SUBTOPIC", "description": "Using confidence bounds to choose actions intelligently.", "source_id": "chunk-d28a8b847f5857eadd394474c6ecf287", "clusters": "[{level: 0, cluster: 0}]", "id": "UPPER-CONFIDENCE-BOUND ACTION SELECTION"}, {"entity_type": "SUBTOPIC", "description": "Bandit problems using gradient methods for selection.", "source_id": "chunk-d28a8b847f5857eadd394474c6ecf287", "clusters": "[{level: 0, cluster: 0}]", "id": "GRADIENT BANDITS"}, {"entity_type": "SUBTOPIC", "description": "A mathematical framework for modeling decision-making scenarios where outcomes are partly random and partly under control of a decision maker.<SEP>Understanding Markov Decision Processes (MDPs) and their role in RL.", "source_id": "chunk-86b5c8bfa1d96abe7c1eff9ab1228fa8<SEP>chunk-d28a8b847f5857eadd394474c6ecf287", "clusters": "[{level: 0, cluster: 2}]", "id": "MARKOV DECISION PROCESSES"}, {"entity_type": "SUBTOPIC", "description": "Details of the interaction between the agent and the environment.", "source_id": "chunk-d28a8b847f5857eadd394474c6ecf287", "clusters": "[{level: 0, cluster: 2}]", "id": "THE AGENT\u2013ENVIRONMENT INTERFACE"}, {"entity_type": "SUBTOPIC", "description": "Importance of setting goals and understanding rewards.", "source_id": "chunk-d28a8b847f5857eadd394474c6ecf287", "clusters": "[{level: 0, cluster: 2}]", "id": "GOALS AND REWARDS"}, {"entity_type": "SUBTOPIC", "description": "Calculation and significance of returns in decision processes.", "source_id": "chunk-d28a8b847f5857eadd394474c6ecf287", "clusters": "[{level: 0, cluster: 2}]", "id": "RETURNS"}, {"entity_type": "SUBTOPIC", "description": "Description of Markov characteristics and why they are important.", "source_id": "chunk-d28a8b847f5857eadd394474c6ecf287", "clusters": "[{level: 0, cluster: 2}]", "id": "MARKOV PROPERTIES"}, {"entity_type": "SUBTOPIC", "description": "The structure and function of MDPs in RL.", "source_id": "chunk-d28a8b847f5857eadd394474c6ecf287", "clusters": "[{level: 0, cluster: 2}]", "id": "MARKOV DECISION PROCESS"}, {"entity_type": "SUBTOPIC", "description": "The calculation and use of value functions to determine optimality.", "source_id": "chunk-d28a8b847f5857eadd394474c6ecf287", "clusters": "[{level: 0, cluster: 2}]", "id": "VALUE FUNCTIONS AND OPTIMAL VALUE FUNCTIONS"}, {"entity_type": "TOPIC", "description": "Comprehensive examination of dynamic programming methods within RL.", "source_id": "chunk-d28a8b847f5857eadd394474c6ecf287", "clusters": "[{level: 0, cluster: 3}]", "id": "DYNAMIC PROGRAMMING"}, {"entity_type": "SUBTOPIC", "description": "Methods for evaluating a given policy.", "source_id": "chunk-d28a8b847f5857eadd394474c6ecf287", "clusters": "[{level: 0, cluster: 3}]", "id": "POLICY EVALUATION (PREDICTION)"}, {"entity_type": "SUBTOPIC", "description": "Techniques to enhance an existing policy.", "source_id": "chunk-d28a8b847f5857eadd394474c6ecf287", "clusters": "[{level: 0, cluster: 3}]", "id": "POLICY IMPROVEMENT"}, {"entity_type": "SUBTOPIC", "description": "An algorithm used in reinforcement learning to find the optimal policy by alternating between policy evaluation and policy improvement.<SEP>The process of iteratively improving policies.", "source_id": "chunk-86b5c8bfa1d96abe7c1eff9ab1228fa8<SEP>chunk-d28a8b847f5857eadd394474c6ecf287", "clusters": "[{level: 0, cluster: 3}]", "id": "POLICY ITERATION"}, {"entity_type": "SUBTOPIC", "description": "Evaluation method for finding the best policy.", "source_id": "chunk-d28a8b847f5857eadd394474c6ecf287", "clusters": "[{level: 0, cluster: 3}]", "id": "VALUE ITERATION"}, {"entity_type": "SUBTOPIC", "description": "Methods for developing solutions asynchronously.", "source_id": "chunk-d28a8b847f5857eadd394474c6ecf287", "clusters": "[{level: 0, cluster: 3}]", "id": "ASYNCHRONOUS DYNAMIC PROGRAMMING"}, {"entity_type": "SUBTOPIC", "description": "General strategies for policy iteration.", "source_id": "chunk-d28a8b847f5857eadd394474c6ecf287", "clusters": "[{level: 0, cluster: 3}]", "id": "GENERALIZED POLICY ITERATION"}, {"entity_type": "TOPIC", "description": "Study of Monte Carlo methods and temporal-difference learning (TD).", "source_id": "chunk-d28a8b847f5857eadd394474c6ecf287", "clusters": "[{level: 0, cluster: 5}]", "id": "MONTE CARLO METHODS AND TEMPORAL-DIFFERENCE LEARNING"}, {"entity_type": "SUBTOPIC", "description": "An approach in reinforcement learning that uses averaged sample returns to estimate the value of actions, with advantages over dynamic programming in certain contexts.<SEP>Predicting outcomes with Monte Carlo techniques.", "source_id": "chunk-86b5c8bfa1d96abe7c1eff9ab1228fa8<SEP>chunk-d28a8b847f5857eadd394474c6ecf287", "clusters": "[{level: 0, cluster: 5}]", "id": "MONTE CARLO PREDICTION"}, {"entity_type": "SUBTOPIC", "description": "Estimating the value of actions through Monte Carlo methods.", "source_id": "chunk-d28a8b847f5857eadd394474c6ecf287", "clusters": "[{level: 0, cluster: 5}]", "id": "MONTE CARLO ESTIMATION OF ACTION VALUES"}, {"entity_type": "SUBTOPIC", "description": "Using Monte Carlo methods for control.", "source_id": "chunk-d28a8b847f5857eadd394474c6ecf287", "clusters": "[{level: 0, cluster: 5}]", "id": "MONTE CARLO CONTROL"}, {"entity_type": "SUBTOPIC", "description": "Prediction using temporal-difference methods.", "source_id": "chunk-d28a8b847f5857eadd394474c6ecf287", "clusters": "[{level: 0, cluster: 5}]", "id": "TD PREDICTION"}, {"entity_type": "SUBTOPIC", "description": "Implementing control with Q-Learning and temporal-difference techniques.", "source_id": "chunk-d28a8b847f5857eadd394474c6ecf287", "clusters": "[{level: 0, cluster: 5}]", "id": "TD CONTROL USING Q-LEARNING"}, {"entity_type": "TOPIC", "description": "Exploration of real-world applications of reinforcement learning techniques in various domains.", "source_id": "chunk-d28a8b847f5857eadd394474c6ecf287", "clusters": "[{level: 0, cluster: 1}]", "id": "APPLICATIONS AND CASE STUDIES"}, {"entity_type": "SUBTOPIC", "description": "Application of RL in optimizing elevator operations.", "source_id": "chunk-d28a8b847f5857eadd394474c6ecf287", "clusters": "[{level: 0, cluster: 1}]", "id": "ELEVATOR DISPATCHING"}, {"entity_type": "SUBTOPIC", "description": "A problem in wireless networks where channels are assigned to users dynamically based on demand, optimized by reinforcement learning for improved network efficiency.<SEP>Use of RL for allocating communication channels dynamically.", "source_id": "chunk-86b5c8bfa1d96abe7c1eff9ab1228fa8<SEP>chunk-d28a8b847f5857eadd394474c6ecf287", "clusters": "[{level: 0, cluster: 1}]", "id": "DYNAMIC CHANNEL ALLOCATION"}, {"entity_type": "SUBTOPIC", "description": "Solving scheduling problems with reinforcement learning.", "source_id": "chunk-d28a8b847f5857eadd394474c6ecf287", "clusters": "[{level: 0, cluster: 1}]", "id": "JOB-SHOP SCHEDULING"}, {"entity_type": "REFERENCE", "description": "Reinforcement Learning: An Introduction, by Richard S. Sutton and Andrew G. Barto.", "source_id": "chunk-d28a8b847f5857eadd394474c6ecf287", "clusters": "[{level: 0, cluster: 4}]", "id": "REINFORCEMENT LEARNING: AN INTRODUCTION"}, {"entity_type": "REFERENCE", "description": "Alessandro Palmas, Dr. Alexandra Galina Petre, and Emanuele Ghelfi, The Reinforcement Learning Workshop: Learn how to Apply Cutting-edge Reinforcement Learning Algorithms to a Wide Range of Control Problems, 2020, Packt Publishing.", "source_id": "chunk-d28a8b847f5857eadd394474c6ecf287", "clusters": "[{level: 0, cluster: 4}]", "id": "THE REINFORCEMENT LEARNING WORKSHOP: LEARN HOW TO APPLY CUTTING-EDGE REINFORCEMENT LEARNING ALGORITHMS TO A WIDE RANGE OF CONTROL PROBLEMS"}, {"entity_type": "REFERENCE", "description": "Phil Winder, Reinforcement Learning Industrial Applications with Intelligent Agents, O\u2019Reilly.", "source_id": "chunk-d28a8b847f5857eadd394474c6ecf287", "clusters": "[{level: 0, cluster: 4}]", "id": "REINFORCEMENT LEARNING INDUSTRIAL APPLICATIONS WITH INTELLIGENT AGENTS"}, {"entity_type": "REFERENCE", "description": "Dr. Engr S M Farrukh Akhtar, Practical Reinforcement Learning, Packt Publishing, 2017.", "source_id": "chunk-d28a8b847f5857eadd394474c6ecf287", "clusters": "[{level: 0, cluster: 4}]", "id": "PRACTICAL REINFORCEMENT LEARNING"}, {"entity_type": "SUBTOPIC", "description": "Part of the prerequisite topic covering probability distributions as essential knowledge for reinforcement learning.", "source_id": "chunk-d28a8b847f5857eadd394474c6ecf287", "id": "PROBABILITY DISTRIBUTIONS"}, {"entity_type": "SUBTOPIC", "description": "Part of the prerequisite topic covering expected values as foundational concepts in reinforcement learning.", "source_id": "chunk-d28a8b847f5857eadd394474c6ecf287", "id": "EXPECTED VALUES"}, {"entity_type": "SUBTOPIC", "description": "Part of the prerequisite topic including inner products, essential for understanding reinforcement learning.", "source_id": "chunk-d28a8b847f5857eadd394474c6ecf287", "id": "BASIC LINEAR ALGEBRA"}, {"entity_type": "SUBTOPIC", "description": "A type of reinforcement learning algorithm discussed under the Reinforcement Learning Algorithms subtopic.<SEP>Q-Learning is an off-policy reinforcement learning algorithm that seeks to find the best action to take given the current state.<SEP>An off-policy reinforcement learning algorithm that seeks to find the best action to take given the current state, and updates the action-value function using a greedy policy.", "source_id": "chunk-86b5c8bfa1d96abe7c1eff9ab1228fa8<SEP>chunk-6a1bb7cfdac4c1f3e0630b5d05d61266<SEP>chunk-d28a8b847f5857eadd394474c6ecf287", "id": "Q-LEARNING"}, {"entity_type": "SUBTOPIC", "description": "An algorithm covered under Reinforcement Learning Algorithms for reinforcement learning purposes.", "source_id": "chunk-d28a8b847f5857eadd394474c6ecf287", "id": "STATE ACTION REWARD STATE ACTION (SARSA)"}, {"entity_type": "SUBTOPIC", "description": "Problems whose characteristics change over time, discussed under Tracking a Nonstationary Problem.", "source_id": "chunk-d28a8b847f5857eadd394474c6ecf287", "id": "NONSTATIONARY PROBLEM"}, {"entity_type": "SUBTOPIC", "description": "Referenced under Dynamic Programming as a prediction method.", "source_id": "chunk-d28a8b847f5857eadd394474c6ecf287", "id": "POLICY EVALUATION"}, {"entity_type": "SUBTOPIC", "description": "Technique for improving an existing policy, related to Policy Improvement under Dynamic Programming.", "source_id": "chunk-d28a8b847f5857eadd394474c6ecf287", "id": "POLICY ENHANCEMENT"}, {"entity_type": "SUBTOPIC", "description": "Defines the iterative improvement strategy of policies.", "source_id": "chunk-d28a8b847f5857eadd394474c6ecf287", "id": "POLICY ITERATION PROCESS"}, {"entity_type": "SUBTOPIC", "description": "Part of Value Iteration seeking the most effective policy.", "source_id": "chunk-d28a8b847f5857eadd394474c6ecf287", "id": "BEST POLICY EVALUATION"}, {"entity_type": "SUBTOPIC", "description": "Refers to methods developed asynchronously within dynamic programming.", "source_id": "chunk-d28a8b847f5857eadd394474c6ecf287", "id": "ASYNCHRONOUS SOLUTION DEVELOPMENT"}, {"entity_type": "SUBTOPIC", "description": "Strategies covered in Generalized Policy Iteration for policy enhancement.", "source_id": "chunk-d28a8b847f5857eadd394474c6ecf287", "id": "GENERAL POLICY STRATEGIES"}, {"entity_type": "SUBTOPIC", "description": "Case studies in applying RL techniques to practical problems in various domains.", "source_id": "chunk-d28a8b847f5857eadd394474c6ecf287", "id": "REAL-WORLD RL APPLICATIONS"}, {"entity_type": "SUBTOPIC", "description": "Examples of scenarios where RL techniques are applied, mentioned in the Applications and Case Studies topic.<SEP>Wide range of control problems addressed using Reinforcement Learning algorithms", "source_id": "chunk-f913ff529fcaac4b3a2487827a8915d3<SEP>chunk-d28a8b847f5857eadd394474c6ecf287", "id": "CONTROL PROBLEMS"}, {"entity_type": "SUBJECT", "description": "Learn how to Apply Cutting-edge Reinforcement Learning Algorithms to a Wide Range of Control Problems", "source_id": "chunk-f913ff529fcaac4b3a2487827a8915d3", "id": "REINFORCEMENT LEARNING WORKSHOP"}, {"entity_type": "AUTHOR", "description": "Author of Reinforcement Learning Industrial Applications with Intelligent Agents", "source_id": "chunk-f913ff529fcaac4b3a2487827a8915d3", "id": "PHIL WINDER"}, {"entity_type": "BOOK", "description": "Features industrial applications of Reinforcement Learning using Intelligent Agents", "source_id": "chunk-f913ff529fcaac4b3a2487827a8915d3", "id": "REINFORCEMENT LEARNING INDUSTRIAL APPLICATIONS WITH INTELLIGENT AGENTS"}, {"entity_type": "AUTHOR", "description": "Author of Practical Reinforcement Learning", "source_id": "chunk-f913ff529fcaac4b3a2487827a8915d3", "id": "DR. ENGR S M FARRUKH AKHTAR"}, {"entity_type": "BOOK", "description": "Packt Publishing\'s guide from 2017 offering a practical approach to learning reinforcement strategies", "source_id": "chunk-f913ff529fcaac4b3a2487827a8915d3", "id": "PRACTICAL REINFORCEMENT LEARNING"}, {"entity_type": "SUBJECT", "description": "The study of how agents can learn optimal behavior through interactions with the environment, often using algorithms<SEP>The study of how agents take actions in an environment to maximize cumulative reward. Focuses on teaching algorithms to achieve optimal strategies via interactions with the environment.", "source_id": "chunk-86b5c8bfa1d96abe7c1eff9ab1228fa8<SEP>chunk-f913ff529fcaac4b3a2487827a8915d3", "id": "REINFORCEMENT LEARNING"}, {"entity_type": "PUBLISHER", "description": "Publisher responsible for releasing educational works related to technology and programming", "source_id": "chunk-f913ff529fcaac4b3a2487827a8915d3", "id": "PACKT PUBLISHING"}, {"entity_type": "PUBLISHER", "description": "Publisher known for high-quality educational content in technology", "source_id": "chunk-f913ff529fcaac4b3a2487827a8915d3", "id": "O\u2019REILLY"}, {"entity_type": "QUESTION", "description": "Define the concepts of goals and rewards in the context of a Markov Decision Process (MDP", "source_id": "chunk-b788b9e24acb11c255a895ef8c7f6cb2", "id": "QUESTION 1F"}, {"entity_type": "QUESTION", "description": "Discuss the Upper-Confidence-Bound (UCB", "source_id": "chunk-b788b9e24acb11c255a895ef8c7f6cb2", "id": "QUESTION 2A"}, {"source_id": "chunk-6a1bb7cfdac4c1f3e0630b5d05d61266", "description": "is related to", "entity_type": "UNKNOWN", "id": "REINFORCEMENT LEARNING ALGORITHMS: Q-LEARNING, SARSA"}, {"entity_type": "TOPIC", "description": "The process of evaluating and improving policies in reinforcement learning to optimize decision making using methods like policy iteration.", "source_id": "chunk-86b5c8bfa1d96abe7c1eff9ab1228fa8", "id": "POLICY EVALUATION AND IMPROVEMENT"}, {"entity_type": "SUBTOPIC", "description": "A method used to evaluate and improve policies in reinforcement learning by comparing on-policy methods like SARSA with off-policy methods like Q-learning.", "source_id": "chunk-86b5c8bfa1d96abe7c1eff9ab1228fa8", "id": "ON-POLICY VS. OFF-POLICY LEARNING"}, {"entity_type": "TOPIC", "description": "Different paradigms in reinforcement learning including model-based and model-free approaches that differ in their utilization of environmental models.", "source_id": "chunk-86b5c8bfa1d96abe7c1eff9ab1228fa8", "id": "TYPES OF REINFORCEMENT LEARNING"}, {"entity_type": "SUBTOPIC", "description": "Two categories of reinforcement learning algorithms focusing on the use or absence of models to predict future states and rewards.", "source_id": "chunk-86b5c8bfa1d96abe7c1eff9ab1228fa8", "id": "MODEL-BASED VS. MODEL-FREE"}, {"entity_type": "SUBTOPIC", "description": "Techniques used to determine the expected utility of following a certain policy in reinforcement learning, often as a preparation step for improvement.", "source_id": "chunk-86b5c8bfa1d96abe7c1eff9ab1228fa8", "id": "POLICY EVALUATION METHODS"}, {"entity_type": "TOPIC", "description": "Core principles of reinforcement learning focusing on elements like Markov Decision Processes and the exploration-exploitation dilemma.", "source_id": "chunk-86b5c8bfa1d96abe7c1eff9ab1228fa8", "id": "FUNDAMENTALS OF REINFORCEMENT LEARNING"}, {"entity_type": "SUBTOPIC", "description": "A problem in which each action provides a randomly determined reward, used to model exploration vs. exploitation strategies in reinforcement learning.", "source_id": "chunk-86b5c8bfa1d96abe7c1eff9ab1228fa8", "id": "MULTI-ARMED BANDITS"}, {"entity_type": "TOPIC", "description": "Exploration of strategies and prediction methods in reinforcement learning, with a focus on techniques like upper-confidence-bound action selection and Monte Carlo methods.", "source_id": "chunk-86b5c8bfa1d96abe7c1eff9ab1228fa8", "id": "MULTI-ARMED BANDITS AND PREDICTION METHODS"}, {"entity_type": "SUBTOPIC", "description": "The trade-off in reinforcement learning between exploring new actions to find effective strategies and exploiting known actions to maximize reward.", "source_id": "chunk-86b5c8bfa1d96abe7c1eff9ab1228fa8", "id": "EXPLORATION-EXPLOITATION"}, {"entity_type": "TOPIC", "description": "Exploration of complex real-world applications and theoretical aspects of reinforcement learning algorithms and strategies.", "source_id": "chunk-86b5c8bfa1d96abe7c1eff9ab1228fa8", "id": "ADVANCED APPLICATIONS AND THEORETICAL CONCEPTS IN REINFORCEMENT LEARNING"}, {"entity_type": "SUBTOPIC", "description": "The application of reinforcement learning techniques to optimize resource allocation and management in wireless communication networks.", "source_id": "chunk-86b5c8bfa1d96abe7c1eff9ab1228fa8", "id": "WIRELESS NETWORKS"}, {"entity_type": "SUBTOPIC", "description": "Theoretical exploration and analysis of foundational concepts within reinforcement learning, such as rewards, returns, and discounting over episodes.", "source_id": "chunk-86b5c8bfa1d96abe7c1eff9ab1228fa8", "id": "CONCEPTUAL EVALUATION"}, {"entity_type": "QUESTION_PAPER", "description": "A collection of questions designed to assess understanding of reinforcement learning principles, evaluations, and applications in a structured academic format.", "source_id": "chunk-86b5c8bfa1d96abe7c1eff9ab1228fa8", "id": "QUESTION PAPER 2024 SEMESTER VIII REINFORCEMENT LEARNING"}, {"entity_type": "DIFFICULTY_LEVEL", "description": "Questions requiring comprehension, analysis, and application of reinforcement learning concepts, typically rated as neither too difficult nor too easy.", "source_id": "chunk-86b5c8bfa1d96abe7c1eff9ab1228fa8", "id": "MEDIUM"}, {"entity_type": "DIFFICULTY_LEVEL", "description": "Higher-level questions demanding synthesis and evaluation skills, expecting students to demonstrate deeper understanding and problem-solving abilities in complex scenarios.", "source_id": "chunk-86b5c8bfa1d96abe7c1eff9ab1228fa8", "id": "HARD"}, {"entity_type": "COGNITIVE_LEVEL", "description": "Reflects the cognitive demand of questions expecting students to comprehend, analyze, and interpret information in reinforcement learning contexts.", "source_id": "chunk-86b5c8bfa1d96abe7c1eff9ab1228fa8", "id": "COMPREHENSION/ANALYSIS"}, {"entity_type": "COGNITIVE_LEVEL", "description": "Questions aimed at assessing students\' retention of knowledge and their ability to apply learned concepts to practical situations in reinforcement learning.", "source_id": "chunk-86b5c8bfa1d96abe7c1eff9ab1228fa8", "id": "KNOWLEDGE/APPLICATION"}, {"entity_type": "COGNITIVE_LEVEL", "description": "Tasks designed to evaluate students\' ability to apply learned theories and conduct thorough analysis within scenarios presented in reinforcement learning.", "source_id": "chunk-86b5c8bfa1d96abe7c1eff9ab1228fa8", "id": "APPLICATION/ANALYSIS"}, {"entity_type": "COGNITIVE_LEVEL", "description": "Expectations for integrating multiple pieces of knowledge and assessing evidence in reinforcement learning to form cohesive judgments or generate innovative solutions.", "source_id": "chunk-86b5c8bfa1d96abe7c1eff9ab1228fa8", "id": "SYNTHESIS/EVALUATION"}, {"entity_type": "QUESTION", "description": "Explain the Policy Improvement Theorem in the context of Reinforcement Learning. Describe the fundamental principle behind the theorem and its proof. Discuss the implications of the theorem on the iterative process of policy iteration.", "source_id": "chunk-86b5c8bfa1d96abe7c1eff9ab1228fa8", "id": "QUESTION 1 (A)"}, {"entity_type": "QUESTION", "description": "Compare SARSA and Q-learning, highlighting the difference between on-policy and off-policy methods. Provide a suitable example.", "source_id": "chunk-86b5c8bfa1d96abe7c1eff9ab1228fa8", "id": "QUESTION 1 (B)"}, {"entity_type": "QUESTION", "description": "Differentiate between model-based and model-free types of Reinforcement Learning (RL). Discuss the advantages and limitations of each approach, providing real-world examples where each type would be most suitable.", "source_id": "chunk-86b5c8bfa1d96abe7c1eff9ab1228fa8", "id": "QUESTION 2 (A)"}, {"entity_type": "QUESTION", "description": "Discuss the Iterative Policy Evaluation with the help of a suitable example.", "source_id": "chunk-86b5c8bfa1d96abe7c1eff9ab1228fa8", "id": "QUESTION 2 (B)"}, {"entity_type": "QUESTION", "description": "Explain the Markov properties and their role in constructing Markov Decision Processes (MDPs) in Reinforcement Learning. Formulate an MDP scenario depicting a bot collecting empty soda cans in an office environment as an illustration of how Markov properties are applied to model complex decision-making tasks.", "source_id": "chunk-86b5c8bfa1d96abe7c1eff9ab1228fa8", "id": "QUESTION 3 (A)"}, {"entity_type": "QUESTION", "description": "Explore Upper-Confidence-Bound (UCB) Action Selection in multi-armed bandits. Analyze UCB\'s formula and address potential application challenges.", "source_id": "chunk-86b5c8bfa1d96abe7c1eff9ab1228fa8", "id": "QUESTION 3 (B)"}, {"entity_type": "QUESTION", "description": "Discuss the k-armed bandit problem, focusing on exploration-exploitation trade-offs. Discuss four practical applications of k-armed bandit problem, across different domains, showcasing its adaptability in optimizing decision-making processes.", "source_id": "chunk-86b5c8bfa1d96abe7c1eff9ab1228fa8", "id": "QUESTION 4 (A)"}, {"entity_type": "QUESTION", "description": "Describe the concept of Monte Carlo Prediction in Reinforcement Learning. Write the pseudocode for first-visit Monte Carlo Prediction. Discuss the advantage of employing Monte Carlo methods over Dynamic Programming (DP) methods specifically in the context of the blackjack game.", "source_id": "chunk-86b5c8bfa1d96abe7c1eff9ab1228fa8", "id": "QUESTION 4 (B)"}, {"entity_type": "QUESTION", "description": "Design a Reinforcement Learning algorithm to optimize Dynamic Channel Allocation in a wireless communication network. Provide the state representation, action space, reward function, and exploration strategy. Discuss any one potential challenge in implementing such an algorithm in a real-world scenario.", "source_id": "chunk-86b5c8bfa1d96abe7c1eff9ab1228fa8", "id": "QUESTION 5 (A)"}, {"entity_type": "QUESTION", "description": "In the context of reinforcement learning, evaluate the concepts of Goals, Rewards, Returns, Episodes and Discounting. Discuss the conventional representations and mathematical formulations associated with Goals, Rewards, Returns, Episodes and Discounting.", "source_id": "chunk-86b5c8bfa1d96abe7c1eff9ab1228fa8", "id": "QUESTION 5 (B)"}, {"entity_type": "SUBTOPIC", "description": "A theorem in reinforcement learning that states improving the policy will improve the value function, thus guiding the iterative process of policy iteration.", "source_id": "chunk-86b5c8bfa1d96abe7c1eff9ab1228fa8", "id": "POLICY IMPROVEMENT THEOREM"}, {"entity_type": "SUBTOPIC", "description": "An on-policy reinforcement learning algorithm that evaluates and improves a policy by updating the action-value function based on the agent\'s actions and experiences from the environment.", "source_id": "chunk-86b5c8bfa1d96abe7c1eff9ab1228fa8", "id": "SARSA"}, {"entity_type": "SUBTOPIC", "description": "An algorithm in reinforcement learning used to calculate the state-value function under policy \u03c0 by successive approximation.", "source_id": "chunk-86b5c8bfa1d96abe7c1eff9ab1228fa8", "id": "ITERATIVE POLICY EVALUATION"}, {"entity_type": "SUBTOPIC", "description": "A strategy in multi-armed bandits that balances exploration and exploitation by selecting actions based on both the average reward and a confidence bound.", "source_id": "chunk-86b5c8bfa1d96abe7c1eff9ab1228fa8", "id": "UCB ACTION SELECTION"}, {"entity_type": "SUBTOPIC", "description": "A reinforcement learning problem that encapsulates the trade-off between exploration and exploitation in decision-making scenarios.", "source_id": "chunk-86b5c8bfa1d96abe7c1eff9ab1228fa8", "id": "K-ARMED BANDIT PROBLEM"}, {"entity_type": "SUBTOPIC", "description": "A collection of algorithms that rely on repeated random sampling to compute results, used in reinforcement learning to evaluate policies and predict outcomes without requiring an explicit Markov model.", "source_id": "chunk-86b5c8bfa1d96abe7c1eff9ab1228fa8", "id": "MONTE CARLO METHODS"}, {"entity_type": "SUBTOPIC", "description": "Concepts in reinforcement learning describing the objective, the immediate and accumulated value received from actions, and the sequence of actions taken.", "source_id": "chunk-86b5c8bfa1d96abe7c1eff9ab1228fa8", "id": "GOALS, REWARDS, RETURNS, AND EPISODES"}, {"entity_type": "SUBTOPIC", "description": "A concept in reinforcement learning that reduces the value of future rewards, reflecting the principle that immediate rewards are often more valuable than future ones.", "source_id": "chunk-86b5c8bfa1d96abe7c1eff9ab1228fa8", "id": "DISCOUNTING"}], "links": [{"weight": 18.0, "description": "is a prerequisite topic for<SEP>is a topic within", "source_id": "chunk-6a1bb7cfdac4c1f3e0630b5d05d61266<SEP>chunk-d28a8b847f5857eadd394474c6ecf287", "order": 1, "source": "ADDO8013 REINFORCEMENT LEARNING", "target": "PREREQUISITE"}, {"weight": 9.0, "description": "is a topic within", "source_id": "chunk-d28a8b847f5857eadd394474c6ecf287", "order": 1, "source": "ADDO8013 REINFORCEMENT LEARNING", "target": "INTRODUCTION TO REINFORCEMENT LEARNING"}, {"weight": 9.0, "description": "is a topic within", "source_id": "chunk-d28a8b847f5857eadd394474c6ecf287", "order": 1, "source": "ADDO8013 REINFORCEMENT LEARNING", "target": "BANDIT PROBLEMS AND ONLINE LEARNING"}, {"weight": 9.0, "description": "is a topic within", "source_id": "chunk-d28a8b847f5857eadd394474c6ecf287", "order": 1, "source": "ADDO8013 REINFORCEMENT LEARNING", "target": "MARKOV DECISION PROCESSES"}, {"weight": 9.0, "description": "is a topic within", "source_id": "chunk-d28a8b847f5857eadd394474c6ecf287", "order": 1, "source": "ADDO8013 REINFORCEMENT LEARNING", "target": "DYNAMIC PROGRAMMING"}, {"weight": 9.0, "description": "is a topic within", "source_id": "chunk-d28a8b847f5857eadd394474c6ecf287", "order": 1, "source": "ADDO8013 REINFORCEMENT LEARNING", "target": "MONTE CARLO METHODS AND TEMPORAL-DIFFERENCE LEARNING"}, {"weight": 9.0, "description": "is a topic within", "source_id": "chunk-d28a8b847f5857eadd394474c6ecf287", "order": 1, "source": "ADDO8013 REINFORCEMENT LEARNING", "target": "APPLICATIONS AND CASE STUDIES"}, {"weight": 7.0, "description": "is a reference for", "source_id": "chunk-d28a8b847f5857eadd394474c6ecf287", "order": 1, "source": "ADDO8013 REINFORCEMENT LEARNING", "target": "REINFORCEMENT LEARNING: AN INTRODUCTION"}, {"weight": 7.0, "description": "is a reference for", "source_id": "chunk-d28a8b847f5857eadd394474c6ecf287", "order": 1, "source": "ADDO8013 REINFORCEMENT LEARNING", "target": "THE REINFORCEMENT LEARNING WORKSHOP: LEARN HOW TO APPLY CUTTING-EDGE REINFORCEMENT LEARNING ALGORITHMS TO A WIDE RANGE OF CONTROL PROBLEMS"}, {"weight": 7.0, "description": "is a reference for", "source_id": "chunk-d28a8b847f5857eadd394474c6ecf287", "order": 1, "source": "ADDO8013 REINFORCEMENT LEARNING", "target": "REINFORCEMENT LEARNING INDUSTRIAL APPLICATIONS WITH INTELLIGENT AGENTS"}, {"weight": 7.0, "description": "is a reference for", "source_id": "chunk-d28a8b847f5857eadd394474c6ecf287", "order": 1, "source": "ADDO8013 REINFORCEMENT LEARNING", "target": "PRACTICAL REINFORCEMENT LEARNING"}, {"weight": 8.0, "description": "is a subtopic of", "source_id": "chunk-d28a8b847f5857eadd394474c6ecf287", "order": 1, "source": "INTRODUCTION TO REINFORCEMENT LEARNING", "target": "REINFORCEMENT LEARNING ALGORITHMS"}, {"weight": 8.0, "description": "is a subtopic of", "source_id": "chunk-d28a8b847f5857eadd394474c6ecf287", "order": 1, "source": "BANDIT PROBLEMS AND ONLINE LEARNING", "target": "AN N-ARMED BANDIT PROBLEM"}, {"weight": 8.0, "description": "is a subtopic of", "source_id": "chunk-d28a8b847f5857eadd394474c6ecf287", "order": 1, "source": "BANDIT PROBLEMS AND ONLINE LEARNING", "target": "ACTION-VALUE METHODS"}, {"weight": 8.0, "description": "is a subtopic of", "source_id": "chunk-d28a8b847f5857eadd394474c6ecf287", "order": 1, "source": "BANDIT PROBLEMS AND ONLINE LEARNING", "target": "TRACKING A NONSTATIONARY PROBLEM"}, {"weight": 8.0, "description": "is a subtopic of", "source_id": "chunk-d28a8b847f5857eadd394474c6ecf287", "order": 1, "source": "BANDIT PROBLEMS AND ONLINE LEARNING", "target": "OPTIMISTIC INITIAL VALUES"}, {"weight": 8.0, "description": "is a subtopic of", "source_id": "chunk-d28a8b847f5857eadd394474c6ecf287", "order": 1, "source": "BANDIT PROBLEMS AND ONLINE LEARNING", "target": "UPPER-CONFIDENCE-BOUND ACTION SELECTION"}, {"weight": 8.0, "description": "is a subtopic of", "source_id": "chunk-d28a8b847f5857eadd394474c6ecf287", "order": 1, "source": "BANDIT PROBLEMS AND ONLINE LEARNING", "target": "GRADIENT BANDITS"}, {"weight": 8.0, "description": "is a subtopic of", "source_id": "chunk-d28a8b847f5857eadd394474c6ecf287", "order": 1, "source": "MARKOV DECISION PROCESSES", "target": "THE AGENT\u2013ENVIRONMENT INTERFACE"}, {"weight": 8.0, "description": "is a subtopic of", "source_id": "chunk-d28a8b847f5857eadd394474c6ecf287", "order": 1, "source": "MARKOV DECISION PROCESSES", "target": "GOALS AND REWARDS"}, {"weight": 8.0, "description": "is a subtopic of", "source_id": "chunk-d28a8b847f5857eadd394474c6ecf287", "order": 1, "source": "MARKOV DECISION PROCESSES", "target": "RETURNS"}, {"weight": 8.0, "description": "is a subtopic of", "source_id": "chunk-d28a8b847f5857eadd394474c6ecf287", "order": 1, "source": "MARKOV DECISION PROCESSES", "target": "MARKOV PROPERTIES"}, {"weight": 8.0, "description": "is a subtopic of", "source_id": "chunk-d28a8b847f5857eadd394474c6ecf287", "order": 1, "source": "MARKOV DECISION PROCESSES", "target": "MARKOV DECISION PROCESS"}, {"weight": 8.0, "description": "is a subtopic of", "source_id": "chunk-d28a8b847f5857eadd394474c6ecf287", "order": 1, "source": "MARKOV DECISION PROCESSES", "target": "VALUE FUNCTIONS AND OPTIMAL VALUE FUNCTIONS"}, {"weight": 8.0, "description": "is a subtopic of", "source_id": "chunk-d28a8b847f5857eadd394474c6ecf287", "order": 1, "source": "DYNAMIC PROGRAMMING", "target": "POLICY EVALUATION (PREDICTION)"}, {"weight": 8.0, "description": "is a subtopic of", "source_id": "chunk-d28a8b847f5857eadd394474c6ecf287", "order": 1, "source": "DYNAMIC PROGRAMMING", "target": "POLICY IMPROVEMENT"}, {"weight": 8.0, "description": "is a subtopic of", "source_id": "chunk-d28a8b847f5857eadd394474c6ecf287", "order": 1, "source": "DYNAMIC PROGRAMMING", "target": "POLICY ITERATION"}, {"weight": 8.0, "description": "is a subtopic of", "source_id": "chunk-d28a8b847f5857eadd394474c6ecf287", "order": 1, "source": "DYNAMIC PROGRAMMING", "target": "VALUE ITERATION"}, {"weight": 8.0, "description": "is a subtopic of", "source_id": "chunk-d28a8b847f5857eadd394474c6ecf287", "order": 1, "source": "DYNAMIC PROGRAMMING", "target": "ASYNCHRONOUS DYNAMIC PROGRAMMING"}, {"weight": 8.0, "description": "is a subtopic of", "source_id": "chunk-d28a8b847f5857eadd394474c6ecf287", "order": 1, "source": "DYNAMIC PROGRAMMING", "target": "GENERALIZED POLICY ITERATION"}, {"weight": 8.0, "description": "is a subtopic of", "source_id": "chunk-d28a8b847f5857eadd394474c6ecf287", "order": 1, "source": "MONTE CARLO METHODS AND TEMPORAL-DIFFERENCE LEARNING", "target": "MONTE CARLO PREDICTION"}, {"weight": 8.0, "description": "is a subtopic of", "source_id": "chunk-d28a8b847f5857eadd394474c6ecf287", "order": 1, "source": "MONTE CARLO METHODS AND TEMPORAL-DIFFERENCE LEARNING", "target": "MONTE CARLO ESTIMATION OF ACTION VALUES"}, {"weight": 8.0, "description": "is a subtopic of", "source_id": "chunk-d28a8b847f5857eadd394474c6ecf287", "order": 1, "source": "MONTE CARLO METHODS AND TEMPORAL-DIFFERENCE LEARNING", "target": "MONTE CARLO CONTROL"}, {"weight": 8.0, "description": "is a subtopic of", "source_id": "chunk-d28a8b847f5857eadd394474c6ecf287", "order": 1, "source": "MONTE CARLO METHODS AND TEMPORAL-DIFFERENCE LEARNING", "target": "TD PREDICTION"}, {"weight": 8.0, "description": "is a subtopic of", "source_id": "chunk-d28a8b847f5857eadd394474c6ecf287", "order": 1, "source": "MONTE CARLO METHODS AND TEMPORAL-DIFFERENCE LEARNING", "target": "TD CONTROL USING Q-LEARNING"}, {"weight": 8.0, "description": "is a subtopic of", "source_id": "chunk-d28a8b847f5857eadd394474c6ecf287", "order": 1, "source": "APPLICATIONS AND CASE STUDIES", "target": "ELEVATOR DISPATCHING"}, {"weight": 8.0, "description": "is a subtopic of", "source_id": "chunk-d28a8b847f5857eadd394474c6ecf287", "order": 1, "source": "APPLICATIONS AND CASE STUDIES", "target": "DYNAMIC CHANNEL ALLOCATION"}, {"weight": 8.0, "description": "is a subtopic of", "source_id": "chunk-d28a8b847f5857eadd394474c6ecf287", "order": 1, "source": "APPLICATIONS AND CASE STUDIES", "target": "JOB-SHOP SCHEDULING"}, {"weight": 7.0, "description": "is related to", "source_id": "chunk-6a1bb7cfdac4c1f3e0630b5d05d61266", "order": 1, "source": "Q-LEARNING", "target": "REINFORCEMENT LEARNING ALGORITHMS: Q-LEARNING, SARSA"}, {"weight": 8.0, "description": "is a topic within", "source_id": "chunk-f913ff529fcaac4b3a2487827a8915d3", "order": 1, "source": "CONTROL PROBLEMS", "target": "REINFORCEMENT LEARNING WORKSHOP"}, {"weight": 5.0, "description": "is referenced in the context of", "source_id": "chunk-f913ff529fcaac4b3a2487827a8915d3", "order": 1, "source": "REINFORCEMENT LEARNING WORKSHOP", "target": "REINFORCEMENT LEARNING INDUSTRIAL APPLICATIONS WITH INTELLIGENT AGENTS"}, {"weight": 7.0, "description": "is referenced for practical approaches", "source_id": "chunk-f913ff529fcaac4b3a2487827a8915d3", "order": 1, "source": "REINFORCEMENT LEARNING WORKSHOP", "target": "PRACTICAL REINFORCEMENT LEARNING"}, {"weight": 9.0, "description": "covers algorithms and applications in", "source_id": "chunk-f913ff529fcaac4b3a2487827a8915d3", "order": 1, "source": "REINFORCEMENT LEARNING WORKSHOP", "target": "REINFORCEMENT LEARNING"}, {"weight": 10.0, "description": "published by", "source_id": "chunk-f913ff529fcaac4b3a2487827a8915d3", "order": 1, "source": "REINFORCEMENT LEARNING WORKSHOP", "target": "PACKT PUBLISHING"}, {"weight": 9.0, "description": "authored", "source_id": "chunk-f913ff529fcaac4b3a2487827a8915d3", "order": 1, "source": "PHIL WINDER", "target": "REINFORCEMENT LEARNING INDUSTRIAL APPLICATIONS WITH INTELLIGENT AGENTS"}, {"weight": 9.0, "description": "applies topics in industrial contexts", "source_id": "chunk-f913ff529fcaac4b3a2487827a8915d3", "order": 1, "source": "REINFORCEMENT LEARNING INDUSTRIAL APPLICATIONS WITH INTELLIGENT AGENTS", "target": "REINFORCEMENT LEARNING"}, {"weight": 10.0, "description": "published by", "source_id": "chunk-f913ff529fcaac4b3a2487827a8915d3", "order": 1, "source": "REINFORCEMENT LEARNING INDUSTRIAL APPLICATIONS WITH INTELLIGENT AGENTS", "target": "O\u2019REILLY"}, {"weight": 9.0, "description": "authored", "source_id": "chunk-f913ff529fcaac4b3a2487827a8915d3", "order": 1, "source": "DR. ENGR S M FARRUKH AKHTAR", "target": "PRACTICAL REINFORCEMENT LEARNING"}, {"weight": 8.0, "description": "provides practical strategies for", "source_id": "chunk-f913ff529fcaac4b3a2487827a8915d3", "order": 1, "source": "PRACTICAL REINFORCEMENT LEARNING", "target": "REINFORCEMENT LEARNING"}, {"weight": 10.0, "description": "published by", "source_id": "chunk-f913ff529fcaac4b3a2487827a8915d3", "order": 1, "source": "PRACTICAL REINFORCEMENT LEARNING", "target": "PACKT PUBLISHING"}]}